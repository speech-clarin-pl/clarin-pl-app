<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="generator" content="Jekyll v3.8.5">
    <title>CLARIN-PL: help</title>

    <link rel="canonical" href="https://getbootstrap.com/docs/4.3/examples/starter-template/">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>

    <style>
        #navbar-example {
            width: 300px;
            position: fixed;
            float: left;
            bottom: 0px;
            top: 0px;
            text-align: left;
            overflow: auto;
            margin-top: 0px;
        }

        #doccontent {
            float: right;
            margin-left: 300px;
            margin-top: 10px;
            padding: 2em;
        }

        /*
        #gornabelka{
            position: fixed;
            width: 100%;
            z-index: 999;
        }
        */

        .nav-pills .nav-link {
            text-decoration: none;
            color: #275c90;
        }

        .nav-pills .nav-link.active,
        .nav-pills .show>.nav-link {
            background-color: #4183c4;
        }
    </style>
</head>

<body style="position: relative" data-spy="scroll" data-target="#navbar-example">

    <!--
        <nav id="gornabelka" class="navbar navbar-dark bg-dark">
                <a class="navbar-brand" href="#">Dokumentacja CLARIN-PL</a>
        </nav>
    -->

    <nav id="navbar-example" class="navbar navbar-light bg-light">
        <nav class="nav nav-pills flex-column">
            <a class="nav-link" href="#wprowadzenie">Wprowadzenie</a>
            <a class="nav-link" href="#korpusy">Korpusy</a>
            <nav class="nav nav-pills flex-column">
                <a class="nav-link ml-3 my-1" href="#korpusStudyjny">Korpus danych studyjnych</a>
                <a class="nav-link ml-3 my-1" href="#korpusPKF">Korpus PKF</a>
                <a class="nav-link ml-3 my-1" href="#korpusSejm">Korpus Sejm</a>
            </nav>
            <a class="nav-link" href="#narzedziamowy">Narzędzia mowy</a>
            <nav class="nav nav-pills flex-column">
                <a class="nav-link ml-3 my-1" href="#g2p">Konwersja zapisu ortograficznego na fonetyczny </a>
                <a class="nav-link ml-3 my-1" href="#alignment">Dopasowanie tekstu do audio</a>
                <a class="nav-link ml-3 my-1" href="#det">Detekcja mowy</a>
                <a class="nav-link ml-3 my-1" href="#vad">Diaryzacja mówców</a>
                <a class="nav-link ml-3 my-1" href="#kws">Wykrywanie słów kluczowych</a>
                <a class="nav-link ml-3 my-1" href="#asr">Automatyczne rozpoznawanie mowy</a>
            </nav>
            <a class="nav-link" href="#analizaKorpusowa">Analiza korpusowa</a>
            <nav class="nav nav-pills flex-column">
                <a class="nav-link ml-3 my-1" href="#przeplywPrac">Przepływ prac</a>
                <a class="nav-link ml-3 my-1" href="#przegladarkaEMU">Przeglądarka EMU-SDMS</a>
                <a class="nav-link ml-3 my-1" href="#jezykR">Język R</a>
                <a class="nav-link ml-3 my-1" href="#przyklad1">Przykład 1</a>
                <a class="nav-link ml-3 my-1" href="#przeplywPrac">Przykład 2</a>
                <a class="nav-link ml-3 my-1" href="#przeplywPrac">Przykład 3</a>
            </nav>
            <a class="nav-link" href="#analizaKorpusowa">Nowy serwis usług mowy</a>
            <nav class="nav nav-pills flex-column">
                <a class="nav-link ml-3 my-1" href="#przeplywPrac">Koncepcja i filozofia</a>
                <a class="nav-link ml-3 my-1" href="#przeplywPrac">Funkcjonalność</a>
                <a class="nav-link ml-3 my-1" href="#przeplywPrac">Plany rozwoju</a>
            </nav>
        </nav>
    </nav>

    <div id="doccontent" data-spy="scroll" data-target="#navbar-example" data-offset="0">

        <h4 id="wprowadzenie">Wprowadzenie</h4>
        <p>
            PJATK, jako jedna z niewielu Uczelni Wyższych w Polsce, zajmuje się analizą sygnału mowy. Celem naszych
            badań jest pomoc osobom zajmującym się naukami humanistycznymi oraz społecznymi w uzyskiwaniu jak
            największej ilości informacji z posiadanych nagrań dźwiękowych (nagrań mowy) oraz informacji tekstowych.
        </p>

        <p>
            Wiele danych wykorzystywanych w naukach humanistycznych i społecznych jest przychowywanych w formie nagrań
            audio. Przykładami mogą być nagrania radiowe bądź telewizyjne, wywiady, przemowy (parlamentu, publiczne
            wystąpienia itp.), wykłady, filmy, literatura czytana i inne nagrania mowy.

            Głównym problemem jednak w przetwarzaniu danych akustycznych jest to iż wymagają znacznie więcej czasu niż
            tradycyjne dane tekstowe. Przytwarzanie tego typu danych wymaga przynajmniej podstawowego "know-how" w
            zakresie przechowywania tego typu danych ale również znacznego wysiłku aby wywnioskować z nagrań informacje
            nadające się do publikacji. Z tego powodu wydobywanie informacji z nagrań dźwiękowych bywa pomijane przez
            badaczy którzy albo nie posiadają wystarczającej ilości czasu lub nie chcą borykać się z w/w problemami.
            Dlatego też naszym głównym celem jest stworzenie darmowych oraz łatwo dostępnych narzędzi dla badaczy z
            dziedziń humanistycznych oraz społecznych.
        </p>

        <p>
            W PJATK budujemy połączenia łączące dane humiastyczne z technologią. Opracowujemy narzędzia informatyczne
            które pomagają porzetwarzać i analizować nagrania mowy. Istotnym celem naszych działań jest ich
            udostępnianie wszystkim którzy chcą wyciągnąć z nich jak najwięcej informacji. Głównym celem wykorzystania
            naszych narzędzi są badania naukowe ale nie tylko. Istotny jest dla nas łatwy dostęp do rozwijanych przez
            nas narzędzi. Dlatego też skupiamy się na tworzeniu łatwo dostępnych oraz zrozumiałych dla każdego
            interfejsów. Wyrazem tego dążenia jest nowy serwis on-line którego celem będzie połączenie wcześnij
            wypracowanych narzędzi w łatwo dostępny interfejs.
        </p>

        <p>
        Staramy się udostępniać nasze usługi za pośrednictwem stron internetowych. Obecnie można mieć do nich dostęp za
        pośrednictwem dwóch serwisów:
        </p>
        <ul>
            <li>
                <a href="https://mowa.clarin-pl.eu" target="_blank">
                    https://mowa.clarin-pl.eu
                </a> (wersja starsza)
            </li>
            <li>
                <a href="https://mowa.clarin-pl.eu:8433/" target="_blank">
                    https://mowa.clarin-pl.eu:8433/
                </a> (nowsza wersja w wersji beta)
            </li>
        </ul>

        <p>Nasz zespół składa się z 5 członków:</p>
        <ul>
            <li>
                <b>Prof. dr hab. Krzysztof Marasek</b>
                Kieronik projektu po stronie PJATK
            </li>
            <li>
                <b>dr inż. Danijel Korzinek</b>
                Analiza i rozpoznawanie mowy
            </li>
            <li>
                <b>dr inż. Łukasz Brocki</b>
                Rozpoznawanie mowy
            </li>
            <li>
                <b>dr inż. Krzysztof Wołk</b>
                Tłumaczenie maszynowe
            </li>
            <li>
                <b>mgr inż. Mariusz Kleć</b>
                Wydobywanie informacji muzycznej (MIR), deep learning, web development
            </li>
        </ul>

        <h4 id="korpusy">Korpusy</h4>

        Nasz zespół zajmuje się tworzeniem oraz przetwarzaniem korpusów nagrań mowy. Korpusy służą do wytrenowania
        określonych modeli oraz wydobywania informacji.

        <h5 id="korpusStudyjny">Korpus danych studyjnych</h5>

        <p>
            Jesteśmy autorem korpusu nagrań studyjnych który udostępniamy <a href="https://mowa.clarin-pl.eu/korpusy"
                target="_blank">pod tym linkiem.</a>. Korpus ten posłużył do wytrenowania systemu rozpoznawania mowy dla
            języka polskiego (bazującego na sytemie Kaldi)
        </p>

        <p>
            Wiele narzędzi wspomnianych wcześniej wymaga dużego zbioru danych trenujących w postaci nagrań audio.
            Pozyskanie dobrej jakości nagrań mowy w danym języku jest kosztowne i często nieosiągalne przez większość
            badaczy. Do tej pory nie istniał darmowy, wysokiej jakości korpus nagrań mowy Polskiej z odpowiednio dużym
            słownikiem.
        </p>

        <p>
            Korpus został nagrany w laboratorium dźwiękowym na terenie PJATK z wykorzystaniem mikrofonów studyjnych.
            Korpus nagrywało 317 mówców w 554 sesjach, gdzie każda sesja składała się z 20 czytanych zdań oraz 10
            fonetycznie bogatych słów pojedyńczych. W sumie zostało nagranych 56 godzin mowy składającej się z 356676
            słów ze słownika o wielkości 46361. Obecnie korpus jest dostępny do <a
                href="https://ips-lmu.github.io/EMU-webApp/?autoConnect=true&serverUrl=wss:%2F%2Fmowa.clarin-pl.eu:17891%2Fclarin"
                target="_blank">łatwego przeglądania on-line za pośrednictwem systemu EMU-SDMS (EMU Speech Database
                Management System). </a>Środowisko EMU pozwala łatwe przeglądanie danych oraz wykonywanie obliczeń
            statystycznych dzięki integracji z środowiskiem R.
        </p>

        <h5 id="korpusPKF">Korpus PKF</h5>

        <p>
            Korpus Polskich Kronik Filmowych uwzględnia nagrania w formie audio oraz wideo z lat 1945-1962. Korpus
            charakteryzuje się archaicznym językiem oraz relatywnie niską jakością nagrań co spowodowało iż był
            wymagającym problemem dla systemu rozpoznawania mowy.
            Korpus zawiera 10 minutowe fragmenty wiadomości, podobne do kronik filmowych wyświetlanych w kinie na całym
            świecie w tym samym okresie. Kronika została stworzona przez Wytwórnie Filmów Dokumentalnych i Fabularnych
            WFDiF w Warszawie. Obecnie prawa do nagrań należą do Filmoteki Narodowej FINA.
        </p>

        <p>
            W komunistycznej Polsce kroniki filmowe były często używane jako narzędzie propagandowe. Zawierają
            komentarze, opinie dziennikarzy, obecne wydarzenia sportowe, ekonomiczne oraz kulturowe z kraju oraz ze
            świata. Przeważnie jedna kronika zawierała pięć części, każda opisująca inny temat ale wszystkie były
            czytane przez jednego mówcę. Jedynie okazjonalnie wtrącane były wywiady oraz przemowy innych ludzi. W
            niektórych przypadkach, takich jak międzynarodowy dzień pracy, cała kronika była podyktowana tylko tym
            wydarzeniom. Oprócz kin, kroniki były również prezentowane w 1960 roku przez Polską telewizję.
        </p>

        <p>
            PKF są wyjątkowo użytecznym źródłem dla środowiska naukowego z powodu ich bogatej zawartości kulturalnej
            oraz historycznej. Dodatkowo dane mogą być annotowane wieloma meta-informacjami jak czas czy miejsce
            opisywanych wydarzeń co stanowi wyjątkowo wartościowe źródło do różnych badań społeczno językowych. Było to
            jednym z powodów dla których zdecydowano się włączyć kroniki do systemu <a
                href="http://chronopress.clarin-pl.eu" target="_blank">ChronoPress</a>. Główną zaletą tego systemu jest
            dodanie informacji czasowych do korpusu. Podczas gdy standardowy korpus może zawierać materiały z różnych
            źródeł oraz z różnych okresów czasowych, użycie takiego korpusu może prowadzić do niepoprawnych wniosków.
            Korpus uporządkowany chronologicznie mówi nam nie tylko jaki jest kontekst wypowiadanych słów ale także
            kiedy były one używane. Pozwala nam to nie tylko uniknąć wyżej wspomnianego problemu ale także dostarcza
            wyjątkowego narzędzia do badania rozwoju języka w czasie. [Obviously, this comes at the additional cost that
            such properly annotated data is much harder to come by than in case of the standard frameworks. While
            discussing the matter with the authors of ChronoPress, it was our intent to help with this particular issue,
            at least partially. The authors of this manuscript are not taking part in the development of the ChronoPress
            platform. Interested parties are kindly requested to send any inquiries with regards to the ChonoPress
            platform to its authors1]
        </p>
        <p>
            Jednym głównym minusem całego projektu była niemożliwość włączenia materiału audio (ze względu na
            ograniczenia licencyjne). Jedynie transkrypcje mogą zostać opublikowane jako te uzyskane z ramach projektu
            CLARIN. Jest to duże ograniczenie ponieważ wierzymy że materiał audio mógłby dostarczyć wyjątkowo bogatego
            źródła do badań fonologii języka Polskiego w tym okresie historycznym.
        </p>
        <p>
            Głównym celem było zatem dostarczenie kolekcji tranksrypcji możliwych do włączenia w system ChronoPress.
            Przy okazji korpus posłużył do stworzenia narzędzia automatycznej transkrypcji które może zostać
            wykorzystane do transkrypcji podobnych nagrań. Aby ułatwić zadanie, zdecydowano iż brane pod uwagę będą
            jedynie dane z okresu 1945-1962 (aby korelowały z innymi zasobami dostępnymi w ChronoPress) oraz jedynie
            głos narratora był trany pod uwagę. Jednym z głównych wyzwań w osiągnięciu tego celu był archaiczny język
            oraz niskiej jakości oraz niestabilny sygnał audio. Dodatkowo, prawie wszystkie filmy cechowała muzyka w tle
            która w momencie trwania mowy była wyciszana ale podgłaszana gdy było jej brak. Innym problemem było
            występowanie wielu języków oraz mówców w niektórych filmach. Pomimo tego korpus posiada wiele zalet. Wiele
            filmów było czytanych przez tą samą grupę dobrze znanych mówców. Dodatkowo domena jest stosunkowo wąska oraz
            dobrze zdefiniowana. Używając kolekcji przetranskrybowanych kronik, oszacowaliśmy wielkość słownika który
            wynosił mniej niż 50k wyrazów. W tym celu przetranskrybowane zostały tyko części zawierające komentatora
            (które stanowiły większość nagrań). Inne części audio (musyka, pozostali mówcy) zostali ignorowani.
        </p>
        <p>
            Wynikowe transkrypcje zostały włączone do <a href="https://clarin-pl.eu/dspace/handle/11321/426"
                target="_blank">repozytorium projektu CLARIN-PL</a>. Wytrenowany model zwraz z systemem do transkrypcji
            oraz segmentacji jest przechowywany w formie obrazu <a href="https://hub.docker.com/u/danijel3/"
                target="_blank">Docker</a>, ze szczegółami dostępnymi w <a
                href="https://github.com/speech-clarin-pl/SpeechToolsWorkers/tree/master/speech_tools"
                target="_blank">publicznym repozytorium.</a>
        </p>

        <h5 id="korpusSejm">Korpus Sejm</h5>

        <p>
            Transkrypcja przemów Parlamentarnych to popularna domena w automatycznym rozpoznawaniu mowy. Rządy wielu
            krajów wymagają transkrypcji oficjalnych spotkań parlamentarzystów oraz ich opublicznienia. Większość tej
            pracy jest ciągle wykonaywana manualnie.

            Obecnie obrady Senatu są dostępne online w postaci audio, video oraz tekstowej.

            Patrząc na przykłady innych, akustycznie podobnych domen do nagrań Parlamentarnych, najbliższe byłyby
            nagrania ze spotkań gdzie częste przerywanie przez innych oraz mówienie przez kilka osób jest dość częstym
            zjawiskiem. Z drugiej strony, język oraz używane słowa są kompletnie różne. Bliższym w tym aspekcie byłyby
            nagrania wykładów. Lecz odbywałyby się w znacząco różnych warunkach akustycznych z prawie nie przerywą
            przemową jednego mówcy.

            Widać tutaj iż transkrypcja nagrań sejmowych jest unikatowym zadaniem.

            Podzbiór nagrań audio z Senatu, wraz z ich odpowiadającym transkrypcjom, został wybrany do wytrenowania
            modelu akustycznego oraz językowego.

            Wiele nagrań zostało opublikowanych na stronie <a href="http://senat.gov.pl" target="blank">Senatu</a>, <a
                href="http://sejm.gov.pl" target="blank">Parlamentu</a> oraz na stronie kanału <a
                href="http://www.tvpparlament.pl/" target="blank">TVP Parlament.</a>
        </p>

        <p>
            Przygotowanie korpusu było czasochłonnym zadaniem, ponieważ oficjalne transkrypcje nie zawierały dokładnych
            transkrypcji nagrań audio. Były pisane w sposób bardziej odpowiadający poprawności gramatycznej bez zmiany
            semantyki wypowiedzi. Transkrypcje tego typu zostały przygotowane przez osoby zatrudnione specjalnie do tego
            celu, aby poprawić czytelność pliku PDF, lecz tego typu forma była mniej użyteczna do wytrenowania modelu
            akustycznego. Dlatego też dane musiałybyć prze-transkrybowane przez grupę zatrudnionych na kontrakcie osób.
            Usuwali oni również fragmenty audio zawierające duże fragmenty szumu tła, podwójną oraz niezrozumiałą mowę
            aby ułatwić proces transkrypcji. Duża część danych została również annotowana informacjami o mówcy (nazwa,
            płeć) aby ułatwić normalizację oraz adaptację modelu akustycznego.

            Większość danych zostało pozyskanych dzięki porozumieniu między Polskim Senatem oraz PJATK. Wiele nagrań
            jest często kiepskiej jakości, realizowanych przy użyciu biurkowych mikrofonów. Wprowadza to wiele
            zniekształceń, pogłosu oraz szumu tła. Dodatkowo do przetrzymywania nagrań używany jest stratny format
            plików.

            W sumie, zostało przygotowanych około 95 godzin nagrań zarówno z Sejmu jak i z Senatu, z 488 różnymi mówcami
            (w większości mężczyzn). Ze wszystkich zostało wybranych 10 mówców losowo jako zbiór testowy (około 2
            godziny nagrań).

            Pozyskanie wystarczającej ilości danych tekstowych okazał się być nieco bardziej skomplikowany.
            Elektroniczne korpusy tekstowe są trudne do uzyskania dla języka Polskiego z powodu małej ilości prób
            cyfryzacji prac pisanych oraz restrektycjnych praw autorskich. Dodatkowo, nawet jeżeli istnieją tekstowe
            korpusy, posiadają ograniczony dostęp, zmuszając naukowców do odtwarzania ich własnych zbiorów każdym razem.

            Początkowo, wystarczająco duży korpus transkrypcji został pozyskany dzięki pomocy biura Senatu, obejmującego
            ponad 5 milionów słów. Korpus ten został później rozszerzony transkrypcjami z sesji Sejmowych oraz
            publicznie dostępnej części National Corpus of Polish (NKJP). Dane te wymagały segmentacji oraz normalizacji
            aby rozszerzyć wszystkie liczby oraz skróty do ich mówionych form w odniesieniu do zasad gramatyki. Tego
            typu normalizacja jest z reguły przeprowadzana manualnie jednak przy takim rozmiarze danych niezbędnym było
            stworzenie automatycznego narzędzia uzgadniającego formę danego wyrazu. Zostało to rozwiązane przy użyciu
            autorskiego oprogramowania wytrenowanego na ręcznie przypisanych zasadach gramatycznych przypisanych do
            rozszerzeń numerycznych. Testy pokazały 10% współczynnik pomyłek.

            Podsumowując, większość eksperymentów zostało przeprowadzonych na korpusie zawierającym około 145 milionów
            wyrazów. Korpus został zamieniony na małe litery a wszystkie liczby i skróty rozszerzone do ich pełnych,
            wypowiadanych gramatycznie form.

        </p>

        <p>
            Celem było wsparcie już istniejącego systemu preznetującego podpisy filmowe na stronie Polskiego senatu.
            Strona ta wykorzystuje nagrania wideo oraz transkrypcje jako osobne dokumenty [Podać link i przykład z
            użyciem].
            Nowo stworzony system może być użyteczny jako w pełni funkcjonalne narzędzie do automatycznego wyświetlania
            transkrypcji, do czasu gdy manualne transkrypcje nie staną się dostępna (zwykle 1-2 dni później).

            Ponadto może być użyty jako narzędzie segmentacji do przypisywania kodów czasowych do już istniejących
            manualnych transkrypcji.

            Największy problem z segmentacją jest taki że manualne transkrypcje nie dokładnie reprezentują co jest
            aktualnie wypowiadane w nagraniu. Manualne transkrypcje są poprawiane w celu ich łatwiejszego czytania w
            pliku PDF ale omijają wiele niuansów mowy jako powtórki wyrazów, wtrącenia itp. Dlatego też, została użyta
            alternatywna technika segmentacji. Po pierwsze audio jest rozpoznane używając modelu języka (wytrenowanego
            na słowniku który występuje w nagraniu audio) a w drugim kroku używane jest dopasowanie text-to-text aby
            określić które części audio zostały poprawnie dopasowane do tekstu. Następnie, ta sama metoda jest używana
            rekursywnie na wszystkich niepoprawnie dopasowanych fragmentacji dopóki nie osiągają perfekcyjnego
            dopasowania. Ta metoda zawsze kończy się "wymuszonym" dopasowanie, jednakże nie zawsze jest to nasz cel.
            Jeżeli transkrypcje które próbujemy dopasować zawierają błędy, możemy chcieć zdecydować aby przerwać
            "wymuszone" dopasowanie wcześniej i zaakceptować wyjście automatycznego rozpoznawania mowy.

            Istnieje wiele technicznych przeszkód w generowaniu podpisów. Rozwiązują to specjalistyczne oprogramowanie,
            stworzone specjalnie na potrzeby telewizyjne bądź kinowe. Programy te zawierają wiele heurystyk aby określić
            optymalne dopasowanie informacji na ekranie w celu łatwego ich śledzenia przez użytkownika. System podpisów
            w projekcie wykorzystującym korpus Sejmowy używa jedynie podstaowych technik (jak np. minimalny czas
            pomiędzy dwoma następującymi po sobie zmianami itp). Bardziej zaawansowane heurysyki byłyby trudne do
            zaimplementowania ponieważ wyjściem systemu rozpoznawania mowy jest ciąg wyrazów którym brakuje granic zdań
            oraz punktuacji. Problem ten jest jednak zaplanowany do rozwiązania w przyszłości.

            Finalny system posiada także możliwość eksportowania wyników do formatu WebVTT. Jest to standard w3C do
            opisywania podpisów używanych na stronach internetowych.
        </p>



        <h4 id="narzedziamowy">Narzędzia mowy</h4>

        <h5 id="g2p">Konwersja zapisu ortograficznego na fonetyczny </h5>
        <p>Narzędie to pozwala na konwersję każdego tekstu napisanego ortograficznie na jego formę fonetyczną (mówioną).
            Jest to jeden z podstawowych kroków w każdych procesie przetwarzania danych mowy. Narzędzie akceptuje każdą
            formę tekstu, jednakże nie wykonuje normalizacji tekstu. Oznacza to iż nie zamienia liczb, dat oraz skrótów
            w sposób automatyczny.
            System jest stworzony przy użyciu systemu opartego o reguły (972 podstawowe reguły oraz 4802 zastępowań
            wyrazów z powodu wyjątków). Dlatego też zawiera listę wyjątków dla nazw zwłasnych, zagranicznych i słów
            nietypowych.
            Narzędzie może generować zarówno listy wyrazów z różnymi wymowami jak również kanoniczną transkrypcję
            tekstu.

            Narzędzie wykorzystuje wariant alfabetu fonetycznego SAMPA, zmodyfikowanego tak aby zawierał tylko litery
            alfabetu (bez symboli jak apostrof czy tylda które zostały zastąpione lietrami i oraz n).

            Ważne aby wspomnieć o wielu możliwych wypowiedziach tego samego słowa, w zależności od zmienności mówcy oraz
            efekty koartykulacji wynikającego z kontektu. Np. [???]
        </p>

        <p>
            Planuje się wdrożenie kilku rozszerzeń do tego narzędzia. Na pierwszym miejscu jest normalizacja tekstu
            (liczb, dat itp.) przed konwersją. Kolejnym rozszerzeniem jest dołączenie różnych form alfabetu fonetycznego
            oraz ewentualnie dodatnie dodatkowych poziomów annotacji (akcenty lub sylabizacja). Nie mniej jednak
            wdrożenie tych rozszerzeń zależy od zainteresowania środowiska wspomnianymi narzędziami.
        </p>

        <p>
            Transkrypcja to zapis wymowy danego słowa. Alfabet ortograficzny nie jest w stanie pełnić tej funkcji,
            ponieważ zapis ortograficzny nie mówi (wbrew pozorom) jak dokładnie należy przeczytać dane słowo. Ponadto
            mnogość alfabetów (łaciński, cyrylica, koreański i inne) wymagałaby znajomości każdego z nich, by móc
            przeczytać słowo z danego języka. Trzeba jednak zauważyć, że chociaż istnieje międzynarodowy alfabet
            fonetyczny (międzynarodowy system API – Alphabet phonétique international), nie zawsze jest on powszechnie
            stosowany. Międzynarodowy system transkrypcji IPA (International Phonetic Alphabet) był tworzony w oparciu o
            fonetykę i fonologię języków zachodnioeuropejskich i nie jest zbyt dobrze dostosowany do języka polskiego.


        </p>

        <h5 id="alignment">Dopasowanie tekstu do audio</h5>
        <p>Tzw. "Speech alignment" jest jednym z bardziej użytecznych narzędzi. Jest używany do dopasowania sekwencji
            słów do dostarczonego nagrania audio zawierającego mowę. Wynik narzędzia może być rozumiany jak automatyczne
            generowanie kodów czasowych gdy znane są zarówno audio jak i jego transkrypcja.
            Jest to o tyle użyteczne narzędzie ponieważ może być użyte do łatwego przeszukiwania konkretnych zdarzeń w
            dużych zbiorach nagrań. Umożliwia takżę obliczanie statystyk odnoszących się do czasu poszczególnych zdarzeń
            (oraz innych ich charakterystyk).
        </p>
        <p>
            Dopasowanie jest wykonywane zarówno na poziomie słów oraz fonemów. Obecnie narzędzie generuje wyjście w
            formacie TextGrid (natywnym dla programu Praat) lecz planuje się wdrożenie innych formatów.
            Narzędzie generuje również link to przeglądarki EMU-webApp która umożliwia przeglądanie rezultatów
            segmentacji bezpośrednio w przeglądarce www.
        </p>

        <p>
            W ramach rozszerzeń planuje się implementację lepszego modelu akustycznego. Adaptacja modelu akustycznego
            wraz z modelem języka byłaby również korzystna, szczególnie jeśli chodzi o zaszumione dane. Narzędzie działa
            poprawnie dla czystych i przewidywalnych danych. Może jednak produkować błędy bądź całkowicie zawieść dla
            sygnałów bardzo zaszumionych bądź o niskim poziomie energii.
            Rozszerzenie interfejsu byłoby również bardzo korzystne. Umożliwiałoby manualną poprawę dopasowania oraz
            poprawy niektórych informacji (np. transkrypcji ortograficznej bądź fonetycznej). Tego typu usprawnienia
            zmieniłyby obecny w pełni automatyczne narzędzie na semi-automatyczne.
        </p>

        <h5 id="det">Detekcja mowy VAD</h5>
        <p>Deatakcja mowy (Voice activity detection: VAD) jest często używane na etapie pre-processingu do wielu
            narzędzi przetwarzania mowy. Ponieważ dane audio są zwykle nie mongeniczne oraz zawierają zmiksowane
            fragmenty mowy, muzyki, tła oraz ciszę. Rozróżnienie pomiędzy tymi różnymi typami audio jest niezwykle
            istotne w skuteczności systemu do transkrypcji.


            Jego celem jest odizolowanie części zawierających mowę od części zawierających inny typ zdarzeń (cisza,
            szum, muzyka itp.).

            Narzędzie to jest kompletnie niezależne od języka oraz domeny wypowiedzi. Nie mnie jednak może generować
            błędy przy bardzo zaszumionych danych. Niewielki eksperyment potwierdził wysoki poziom czułości (Recall ~
            99%) oraz średnią precyzję (Precision ~ 58%). Było to jednak zamierzonym celem aby nie utracić żadnych
            części zawierających mowę, akecptując czasami fragmenty które jej nie zawierają. Jest to spowodowane tym iż
            pozostałe narzędzia akceptują niewielką ilość zaszumionych danych, ale działają błędnie gdy jakakolwiek
            część mowy jest pominięta.</p>

        <h5 id="vad">Diaryzacja mówców</h5>
        <p>Narzędzie to jest używane do segmentacji dużych plików audio na części wypowiadane przez poszczególne osoby.
            Istnieje kilka typów strategii segmentacji mówców. Pierwsza to rozpoznawanie momentu zmiany mówcy na innego,
            druga to dodanie informacji który fragment należy do tego samego mówcy oraz trzecia strategia polega na
            identyfikacji rozpoznanych fragmentów tak aby wiedzieć kto dokładnie mówi w rozpoznanym segmentcie. Nasze
            narzędzie wspiera jednak drugą strategię w której rozpoznajemy zmiany mówców, wiemy ilu ich jest oraz w
            jakich momentach nagrania występując. Narzędzie jednak traktuje mówców w sposób anonimowy. Narzędzie to jest
            użyteczne do adaptacji różnych narzędzi oraz modeli do indywidualnych mówców ale również do innych typów
            anali które wymagają segmentacji mówców.</p>

        <h5 id="kws">Wykrywanie słów kluczowych</h5>
        <p>
            Często dokładna transkrypcja materiału audio nie jest konieczna ponieważ jesteśmy zainteresowani tylko
            występowaniem pojedyńczych słów. Wykrywanie słów kluczowych jest procesem który pobiera plik audio oraz
            listę słów kluczowych. Następnie generuje listę występowania tych słów w obrębie pliku audio. Należy jednak
            zwrócić uwagę że model języka ma ograniczoną wielkość słownika, dlatego też niemożliwym jest przewidzieć
            wszystkich możliwych wyrazów. Z tego powodu, system używa kombinacji słów oraz wyrazów w taki sposób że gdy
            jest potrzeba znalezienia słowa spoza słownika, używana jest reprezentacja sylabowa danego słowa. Dzięki
            temu system radzi sobie ze słowami które są spoza słownika, ale jest bardziej podatny na błędy gdy
            dostarczone są bardzo krótkie słowa kluczowe. W celu przetestowania narzędzia został przygotowany test który
            pokazał całkowitą precyzję na poziomie ~95% oraz poziom czułości (Recall) dla znanych wyrazów ~82% oraz
            niski poziom dla wyrazów nieznanych (~20%). Model oparty o sylaby wymaga poprawy w przyszłości aby uniknąć
            błędów w przypadku wyrazów nieznanych.
        </p>

        <h5 id="asr">Automatyczne rozpoznawanie mowy</h5>

        <p>Narzędzie to używa systemu rozpoznawania mowy do wygenerowania najbardziej prawdopodobnyej ortograficznej
            transliteracji nagrań dźwiękowych mowy Polskiej. Prawidłowo zaimplementowany system zawiera kilka
            komponentów:</p>
        <ul>
            <li>Moduł transkrypcji fonetycznej, znany jako tzw. konwerter zapisu ortograficznego na fonetyczny
                (graphene-to-phoneme G2P) </li>
            <li>Detektor występowania mowy (voice activity detector VAD) </li>
            <li>Model akustyczny </li>
            <li>Model języka </li>
            <li>dekoder </li>
        </ul>

        <p>
        Na początku sygnał audio jest poddawany ekstrakcji cech w postaci ramek czasowych. Użyty został standardowy
        zestaw 39 cech (głównie MFCC) z 100 ramkami na sekundę 25ms nakładaniem się.

        Następnie ramki są filtrowane używając modułu VAD oraz dostarczone do modelu akustycznego. Informacje zebrane z
        modelu akustycznego oraz z modelu języka są przetwarzane przez dekoder do generowania ciągu słów. Słowa te są
        określone przez słownik który jest mapowany do wyjścia fonetycznego modelu akustycznego używając modułu G2P.

        Konwersja G2P jest istotnym pomostem pomiędzy dźwiękiem mowy oraz jak jej używamy podczas czytania i pisania
        słów na papierze. Procedura ta jest ważna zarówno w fazie trenowania (do konwersji trenujących transkrypcji na
        deskrypcje rozpoznające dźwięki mowy) oraz podczas normalnego użycia (do konwersji rozpoznanych dźwięków na text
        do czytania).
        </p>


        <h4 id="analizaKorpusowa">Analiza Korpusowa</h4>
        <h5 id="przeplywPrac">Przepływ prac</h5>
        <p>
            to do
        </p>


        <h5 id="przegladarkaEMU">Przeglądarka EMU-SDMS</h5>
        <p>
            System zarządzania bazami mowy (EMU Speech Database Management System EMU-SDMS) jest zbiorem narzędzi
            których celem jest bycie jak najbliżej rozwiązaniom typu all-in-one do generowania, manimulacji, zapytań
            oraz analizy oraz zarządzania nagraniami audio zawierającymi mowę.

            System jest skoncentrowany wokół języka R umożliwiającego obliczanie statystyk. System zawiera 4 główne
            komponenty: wrassp, emuR, emuDB oraz EMU-webApp. Te 4 komponenty pomagają w przeprowadzaniu badań
            dotyczących mowy oraz języka naturalnego, poprzez dostarczenie zintegrowanego systemu dzięki któremu
            jesteśmy w stanie odpowiedzień na następujące pytanie badawcze: biorąc pod uwagę daną bazę danych, czy na
            wysokość samgłoski @ (mierzoną przez jej pierwszy formant) ma wpły to, czy pojawia się one w silnej czy
            słabej sylabie?
        </p>

        <img src="images/overview.png" alt="overview EMU" />

        <p>
            Trzeba zauważyć że pakiet emuR jest jedynym komponentem który komunikuje się ze wszystkimi innymi. Z tego
            też powodu jest najistotniejszy. Zajmuje się obsługą plików oraz bazy danych. Używa pakietu wrassp do
            przetwarzania sygnałów oraz dostarcza resultaty do EMU-webApp. Pomimo iż system składa się z 4 komponentów,
            urzytkownik używa przedewszystkim Emu-webApp oraz emuR.
        </p>
        <p>
            Domyślny schemat przepływu pracy wygląda następująco:
        </p>
        <ol>
            <li>Załadowanie bazy danych do sesji R (load_emuDB())</li>
            <li>Annotacja bazy / wizualna inspekcja (server()). Operacja ta otwiera EMU-webApp w przeglądarce</li>
            <li>Zapytanie do bazy danych (query()). Opcjonalnie przy użyciu funkcji requery_hier() lub requery_seq</li>
            <li>Zbieranie danych śledzących (np. wartości formantów) dla wyników zapytania</li>
            <li>Przygotowanie danych</li>
            <li>Wizualna inspekcja danych</li>
            <li>Przeprowadzenie dalszych analiz oraz przetwarzania statystycznego</li>
        </ol>
        <p>Początkowo użytkownik tworzy referencję do emuDB poprzez funkcję load_emuDB(). Tak uzyskana referencja do
            bazy może zostać użyta albo do wyświeltania jej w EMU-webApp (serve()) lub do tworzenia zapytań (query()) do
            annotacji. Rezultaty zapytania mogą wtedy być użyte albo do wykonania jednego lub więcej tzw podzapytań
            (requeries) lub do wyekstrachowania wartości sygnału które odpowiadają wynikowi zapytania. Finalnie, dane
            sygnału mogą przejść dalsze przygotowania (np. usuwanie outliers) oraz sprawdzone wizualnie przez dalszą
            analizą oraz przetwarzaniem statystycznym. Wynikowy obiekt powyższych funkcji (matrix lub data.frame) może
            być użyty jako wejście do tysięcy innych funkcji języka R. EMU-SDMS znacząco redukuje liczbę narzędzi mowy i
            języka z którymi badacze muszą się zmagać oraz pomaga ułatwić odpowiedzi na pytania badawcze. Jedynym
            wymogiem w użyciu jest podstawowa znajomość platformy R.</p>

        <p>
            Aby lepiej zobrazować zasadę działania systemu przeprowadzimy wszystkie procesy krok-po-kroku aby odpowiedzieć na pytanie badawcze: "mając dany zbiór danych, wysokość samogłoski @ jest zależna od tego czy występuje w wyrazie czy jako znak funkcyjny?"

            Gdy nagrania audio są dostępne należy przekształcić jes na format emuDB. Istnieje kilka metod (udostępnionych przez emuR) które umożliwią nam zaimportowanie kolekcji plików audio wraz z ich transkrypcją w formacie np. TextGrid. Służy do tego funkcja convert_ToxtGridCollection(). Oczywiście istnieją
            również inne funkcje ułatwiające ten proces. Ich opis jest dostępny w <a
                href="https://ips-lmu.github.io/The-EMU-SDMS-Manual/chap-tutorial.html"
                target="_blank">dokumentacji</a>. Aby dowiedzieć się więcej na temat działania tej funkcji można wpisać
            help(convert_ToxtGridCollection).

            W EMU-SDMS, kolekcja plików taka jak kolekcja TextGrid odnosi się do zestawu par plików gdzie występują dwa
            typy plików o różnych rozszerzeniach ale o tej samej nazwie podstawowej. Jest to ważne aby system był w
            stanie sparować pliki razem ze sobą.

            Funkcja convert_ToxtGridCollection() przekonwertuje kolekcje TextGrid na format emuDB. Założeniem wstępnym
            jest to że pliki .TextGrid muszą zawierać te same poziomy annotacji. Jeżeli warunek nie jest spełniony,
            można wybrać tylko ten podzbiór poziomów annotacji który jest wspólny dla wszystkich plików. Można
            oczywiście wybrać które poziomy annotacji chcemy brać pod uwagę. Po stworzeniu przez funkcje bazy danych
            możemy załadować naszą bazę do sesji R używając funkcji load_emuDB().

            Dalej za pomocą funkcji summary() możemy podejrzeć strukturę bazydanych.

            Po załadowaniu przychodzi czas na wiaualną inspekcję danych w EMU-webApp. Służy to tego funkcja server().

            Integralnym krokiem w domyuślnym prezpływie pracy są zapytania do bazy danych. emuR implementuje metodę
            query() do tego celu. Funkcja ta wykonuje wyrażenia EMU Query Language (EQL) oraz wydobywa zaanotowane
            fragmenty z bazy danych które spełniają wymagania zapytania.

            Możemy wykonywać zapytania osobno dla każdego poziomu np aby zwrócił wszystkie fragmenty z poziomu "Syllabe"
            które są równe "S" (silna sylaba). W tym celu wykonalibyśmy polecenie:

            <!--
            <code>
sl_syl = query(emuDBhandle = db_handle,
                query = "Syllable == S")
    </code>
-->

            Rezultatem takiego zapytania jest obiekt emuRsegs, który jest superklasą data.frame. Obiekt ten często
            odnosi się do listy segmentów które są opisane za pomocą punktu startowego oraz końcowego segmentów, z
            jakiej sesji oraz paczki pochodzi dany wpis oraz poziom do którego należy.

            Tego typu zapytania pochodzą z pojedyńczego poziomu który pasuje do określonej etykiety. Jednakże EMU-SDMS
            oferuje mechanizm do wykonywania wewnątrz poziomowych zapytań takich jak: zwróć wszystkie pozycje z poziomu
            Phonetic które zawierają etykietę "n" oraz są częścią zawartości wyrazu. Aby to było możliwe, EMU-SDMS
            oferuje dość zaawansowane funkcje modelowania struktury adnotacji.

            Aby to było możliwe należy przekształcić płaską strukturę annotacji na hierarchiczną jak zaprezentowano na
            poniższym rysunku.

            <img src="images/tutorial-violentlyHier-1.png" alt="" />

            Warto zauważyć że tylko poziom "Phonetic" zawiera informacje o czasie, a pozycje na innych poziomach są
            połączone ze sobą w formę hierarchicznej struktury.

            Manualne tworzenie takich połączeń byłoby zbyt czasochłonnym zadaniem. Hierarchiczna informacja jest jednak
            pośrednio zawarta z czasowych przedziałach semgnetów na każdym poziomie. Należy więc użyć funkcji
            automatycznej (dostarczonej przez emuR) do zbudowania tej struktury w sposób automatyczny. Służy do tego
            funkcja autobuild_linkFromTimes(). Należy to jednak robić poziom po poziomie, zaczynając od najbardziej
            ogólnych połączeń (np. wyrazy-sylaby) do bardziej szczegółowych (np. sylaby-fonemy).

            po utworzeniu struktury można wyświetlić ja w Emu-Webapp dzięki komendzie server(). W Emu-Webapp możemy
            zobaczyć stworzoną strukturę klikając przycisk "show hierarchy":


        </p>

        <img src="images/EMU-webAppScreenshotTutorialPostAutobHier.png" alt="" style="width: 100%" />

        <h6>Zapytania do hierarchicznej annotacji</h6>

        <p>
            Hierarchiczna struktura annotacji pozwala na formułowanie zapytań które pomogą odpowiedzieć na pytanie
            badawcze (wysokość samogłoski @ różni się w zależności od tego czy występuje w wyrazie czy jako znak
            funkcyjny)

            Na początku należy wydobyć wszystkie samogłoski @ z bazy danych z poziomu "Phonetic":
        </p>
        <!--
        <code>
        sl_vowels = query(db_handle, "Phonetic == @")
    </code>
-->
        <p>
            Potrzebujemy jednak typ wyrazu (zwykły bądź funkcyjny) dla którego zostały znalezione samogłoski @. W tym
            celu użyjemy funkcjonalności zwanej "requery" z systemu EMU-SDMS. Dzięki temu wydobędziemy typ wyrazu dla
            każdej samogłoski @. W skrócie, "requery" porusza się przez hierarchiczną annotacje (vertykalnie i
            horyzontalnie) zaczynając od segmentów które zostały przekazane do funkcji "requery".
        </p>

        <!--
        <code>

sl_word_type = requery_hier(db_handle,
                            seglist = sl_vowels,
                            level = "Word",
                            calcTimes = FALSE)  
    </code>
-->

        <p>
            Należy zwrócić uwagę że liczba wierszy w liście segmentów zwróconych przez funkcję requery_hier() jest taka
            sama jak oryginalna lista samogłosek sl_vowels. Jest to istotne, ponieważ każdy rząd w obu listach segmentów
            powinien być połączony, pozwalając nam na wyciąganie które semgnet należy do jakiego typu wyrazu.
        </p>


        <h6>Ekstrakcja cech</h6>
        <p>
            Jeżeli mamy już samogłoski oraz typu wyrazów które zawierają te samogłoski (oraz informacje o ich czasie
            występowania), możemy wyekstrachować cechy sygnału do tych segmentów. Funkcja get_trackdata() oblicza
            wartości formantów w czasie rzeczywistym, używając funkcji estymacji formantów forest(), dostarczonej przez
            pakiet wrassp.
        </p>

        <!--
        <code>
        # get formant values for the vowel segments
        td_vowels = get_trackdata(db_handle,
                                    seglist = sl_vowels,
                                    onTheFlyFunctionName = "forest",
                                    resultType = "tibble",
                                    verbose = F)
        
</code>
-->

        <p>
            funkcja forest() oblicza 4 wartości formantów. Interesują nas jednak tylko wartości 1 i 2 formantu.

            Możemy narysować wykres obrazujący trajektorie pierwszego formantu dla wszystkich znalezionych segmentów:
        </p>

        <!--
        <code>
        ggplot(td_vowels) +
        aes(x = times_rel, y = T1, col = labels, group = sl_rowIdx) +
        geom_line() +
        labs(x = "Duration (ms)", y = "F1 (Hz)")
</code>
-->

        <img src="images/tutorial-dplot1-1.png" alt="" style="width: 100%" />

        <p>
            Jednak aby wyczyścić ten wykres, następny zawiera normalizację długości segmentu oraz metodę wygładzającą
            geom_smooth() aby wyświetlić wygładzoną średnią warunkową wszystkich samogłosek @
        </p>

        <!--
        <code>
        # normalize length of segments
        td_vowels_norm = normalize_length(td_vowels)
        
        ggplot(td_vowels_norm) +
            aes(x = times_norm, y = T1, col = labels, group = labels) +
            geom_smooth() +
            labs(x = "Duration (normalized)", y = "F1 (Hz)") 
    </code>
-->

        <img src="images/tutorial-dplot2-1.png" alt=""  style="width: 100%"/>

        <p>
            Powyższe wykresy dają wgląd w trajektorie wszystkich samogłosek @. Do celów eksploracji danych oraz aby
            uzyskać informację gdzie poszczególne klasy samogłosek leżą w przestrzeni 1 i 2 formantu (które pośrednio
            dostarczają informacji na temat wysokości samogłoski oraz pozycji języka), możemy użyć ponownie funkcji
            ggplot(). Jednakże tym razem nie chcemy mieć już trajektorii wartości formantów ale zależy nam na
            dwówymiarowych danych ay wyświelić je w przestrzeni F2 x F1. Można to uzyskać np. poprzez ekstrakcję
            środkowego formantu dla każdej samogłoski (albo za pomocą parametru cut dla funckcji get_trackdata() lub
            używajć funkcji filter()).
        </p>

        <!--
        <code>
        # cut formant trajectories at temporal mid-point
        td_vowels_midpoint = td_vowels_norm %>% 
            filter(times_norm == 0.5)

            # calculate centroid 
td_centroids = td_vowels_midpoint %>%
    group_by(labels) %>%
    summarise(T1 = mean(T1), T2 = mean(T2))

    # generate plot
ggplot(td_vowels_midpoint, aes(x = T2, y = T1, colour = labels, label = labels)) + 
    geom_text(data = td_centroids) +
    stat_ellipse() +
    scale_y_reverse() + scale_x_reverse() + 
    labs(x = "F2 (Hz)", y = "F1 (Hz)") +
    theme(legend.position="none")
    </code>
-->

        <img src="images/tutorial-eplot-1.png" alt="" style="width: 100%" />

        <p>
            Wykres powyżej pokazuje pierwsze dwa formanty wyektrachowane z środka każdej samogłoski @. Centroid tych
            formantów został pokazany na wykresie F2 x F1 oraz ich 95% dystrybucja.

            Powyższe przykłady nie pomagają wprost odpowiedzieć na pytanie zawarte we wstępie, jednak pokazują
            użyteczność pakietów dplyr oraz ggplot2 że są użytecznymi narzędziami do dostarczania wglądu w dane.
        </p>

        <h6>Wysokość samogłoski jako funkcja typu wyrazu</h6>

        <p>

            Powyższe analizy dotyczyły jedynie samogłoski @ bez odniesienia do typu wyrazu w którym występowała.
            Jednakże pytanie badawcze uwzględnia ten kontekst.

            Możemy więc wyektrachować 60% formantów z ich traijektorii oraz wyświetlić używając ggplot(). Jednakże
            chcemy je pogrupować ze względu na typ wyrazu w którym występują. W tym celu zastąpimy etykiety
            td_vowels_mid_sec etykietami sl_word_type:
        </p>

        <!--
        <code>
        # extract central 60% from formant trajectories
        td_vowels_mid_sec = td_vowels_norm %>% 
            filter(times_norm >= 0.2, times_norm &lt= 0.8)
        
        # replace labels with those of sl_word_type
        td_vowels_mid_sec$labels = sl_word_type$labels[td_vowels_mid_sec$sl_rowIdx]
        
        ggplot(td_vowels_mid_sec) +
            aes(x = times_norm, y = T1, col = labels, group = labels) +
            geom_smooth() +
            labs(x = "Duration (normalized)", y = "F1 (Hz)") 
        </code>
    -->

        <img src="images/tutorial-dplotSylTyp-1.png" alt="" style="width: 100%" />

        <p>

            Jak można zauważyć, wydaje się różnica między wysokością F1 samogłosek występujących samodzielnie oraz w
            wyrazie.

            Aby lepiej to zobrazować możemy wygenerować inny wykres

        </p>

        <!--
        <code>
        # use group_by + summarise to calculate the means of the 60%
        # formant trajectories
        td_vowels_mid_sec_mean = td_vowels_mid_sec %>%
            group_by(sl_rowIdx) %>%
            summarise(labels = unique(labels), meanF1 = mean(T1))
        
        
        # create boxplot using ggplot
        ggplot(td_vowels_mid_sec_mean, aes(labels, meanF1)) +
            geom_boxplot() +
            labs(x = "Word type", y = "mean F1 (Hz)")
</code>
-->

        <img src="images/tutorial-boxplot-1.png" alt="" style="width: 100%" />

        <p>
            Więcej przykładów oraz wyczerpujący opis użycia EMU-SDMS do rozwiązywania problemów badawczych można znaleźć
            w <a href="https://ips-lmu.github.io/The-EMU-SDMS-Manual/chap-tutorial.html"
                target="_blank">dokumentacji</a>.
        </p>


    </div>


    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="/docs/4.3/assets/js/vendor/jquery-slim.min.js"><\/script>')</script>
    <script src="/docs/4.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-xrRywqdh3PHs8keKZN+8zzc5TX0GRTLCcmivcbNJWm2rs5C8PRhcEn3czEjhAO9o"
        crossorigin="anonymous"></script>
</body>

</html>