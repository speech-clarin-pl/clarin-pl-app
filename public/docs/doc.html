<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="generator" content="Jekyll v3.8.5">
    <title>CLARIN-PL: help</title>

    <link rel="canonical" href="https://getbootstrap.com/docs/4.3/examples/starter-template/">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>

    <style>
        #navbar-example {
            width: 300px;
            position: fixed;
            float: left;
            bottom: 0px;
            top: 0px;
            text-align: left;
            overflow: auto;
            margin-top: 0px;
        }

        #doccontent {
            float: right;
            margin-left: 300px;
            margin-top: 10px;
            padding: 2em;
        }

        /*
        #gornabelka{
            position: fixed;
            width: 100%;
            z-index: 999;
        }
        */

        .nav-pills .nav-link {
            text-decoration: none;
            color: #275c90;
        }

        .nav-pills .nav-link.active,
        .nav-pills .show>.nav-link {
            background-color: #4183c4;
        }
    </style>
</head>

<body style="position: relative" data-spy="scroll" data-target="#navbar-example">

    <!--
        <nav id="gornabelka" class="navbar navbar-dark bg-dark">
                <a class="navbar-brand" href="#">Dokumentacja CLARIN-PL</a>
        </nav>
    -->

    <nav id="navbar-example" class="navbar navbar-light bg-light">
        <nav class="nav nav-pills flex-column">
            <a class="nav-link" href="#wprowadzenie">Wprowadzenie</a>
            <a class="nav-link" href="#korpusy">Korpusy</a>
            <nav class="nav nav-pills flex-column">
                <a class="nav-link ml-3 my-1" href="#korpusStudyjny">Korpus danych studyjnych</a>
                <a class="nav-link ml-3 my-1" href="#korpusPKF">Korpus PKF</a>
                <a class="nav-link ml-3 my-1" href="#korpusSejm">Korpus Sejm</a>
            </nav>
            <a class="nav-link" href="#narzedziamowy">Narzędzia mowy</a>
            <nav class="nav nav-pills flex-column">
                <a class="nav-link ml-3 my-1" href="#g2p">Konwersja zapisu ortograficznego na fonetyczny </a>
                <a class="nav-link ml-3 my-1" href="#alignment">Dopasowanie tekstu do audio</a>
                <a class="nav-link ml-3 my-1" href="#det">Detekcja mowy</a>
                <a class="nav-link ml-3 my-1" href="#vad">Diaryzacja mówców</a>
                <a class="nav-link ml-3 my-1" href="#kws">Wykrywanie słów kluczowych</a>
                <a class="nav-link ml-3 my-1" href="#asr">Automatyczne rozpoznawanie mowy</a>
            </nav>
            <a class="nav-link" href="#analizaKorpusowa">Analiza korpusowa</a>
            <nav class="nav nav-pills flex-column">
                <a class="nav-link ml-3 my-1" href="#przegladarkaEMU">Przeglądarka EMU-SDMS</a>
                <a class="nav-link ml-3 my-1" href="#przeplywPracy">Etapy pracy badawczej</a>
                
            </nav>
            <a class="nav-link" href="#przykladyAnaliz">Przykłady analiz</a>
            <nav class="nav nav-pills flex-column">
                <a class="nav-link ml-3 my-1" href="#przyklad1">Przykład 1: Wizualizacja samogłosek</a>
                <a class="nav-link ml-3 my-1" href="#przyklad2">Przykład 2: F0 zależne od płci</a>
                <a class="nav-link ml-3 my-1" href="#przyklad3">Przykład 3: Głoski dźwięczne i bezdźwięczne </a>
                <a class="nav-link ml-3 my-1" href="#przyklad4">Przykład 3: VSA i VAI</a>
            </nav>
            <a class="nav-link" href="#analizaKorpusowa">Nowy serwis usług mowy</a>
            <nav class="nav nav-pills flex-column">
                <a class="nav-link ml-3 my-1" href="#przeplywPrac">Koncepcja i filozofia</a>
                <a class="nav-link ml-3 my-1" href="#przeplywPrac">Funkcjonalność</a>
                <a class="nav-link ml-3 my-1" href="#przeplywPrac">Plany rozwoju</a>
            </nav>
        </nav>
    </nav>

    <div id="doccontent" data-spy="scroll" data-target="#navbar-example" data-offset="0">

        <h4 id="wprowadzenie">Wprowadzenie</h4>
        <p>
            PJATK, jako jedna z niewielu Uczelni Wyższych w Polsce, zajmuje się analizą sygnału mowy. Celem naszych badań jest pomoc osobom zajmującym się naukami humanistycznymi oraz społecznymi w uzyskiwaniu jak największej ilości informacji z posiadanych nagrań dźwiękowych (nagrań mowy) oraz informacji tekstowych.
        </p>

        <p>
            Wiele danych wykorzystywanych w naukach humanistycznych i społecznych jest przechowywanych w formie nagrań audio. Przykładami mogą być nagrania radiowe bądź telewizyjne, wywiady, przemowy (parlamentu, publiczne wystąpienia itp.), wykłady, filmy, literatura czytana i inne nagrania mowy. 
            Głównym problemem jednak w przetwarzaniu danych akustycznych jest to iż wymagają znacznie więcej czasu niż tradycyjne dane tekstowe. Przetwarzanie tego typu danych wymaga przynajmniej podstawowego "know-how" w zakresie przechowywania tego typu danych ale również znacznego wysiłku aby wywnioskować z nagrań informacje nadające się do publikacji. Z tego powodu wydobywanie informacji z nagrań dźwiękowych bywa pomijane przez badaczy którzy albo nie posiadają wystarczającej ilości czasu lub nie chcą borykać się z w/w problemami. Dlatego też naszym głównym celem jest stworzenie darmowych oraz łatwo dostępnych narzędzi dla badaczy z dziedzin humanistycznych oraz społecznych. 
            Celem naszych badań jest pomoc w uzyskaniu jak największej ilości informacji z posiadanych nagrań dźwiękowych (nagrań mowy) oraz informacji tekstowych. W PJATK budujemy połączenia łączące dane humiastyczne z technologią.
        </p>

        <p>
            Tworzenie korpusów oraz wydobywanie z nich informacji naturalnie wymaga narzędzi. Istotny jest dla nas łatwy dostęp do nich dostęp. Dlatego też obecnie skupiamy się również na tworzeniu łatwo dostępnych oraz zrozumiałych  interfejsów, aby narzędzia były dostępne dla każdego, nie tylko dla informatyków. Wyrazem tego dążenia jest obecny oraz nowy serwis www udostępniające opisywane dalej narzędzia mowy.  Obecnie można mieć do nich dostęp za pośrednictwem dwóch serwisów:
        </p>

        <ul>
            <li>
                <a href="https://mowa.clarin-pl.eu" target="_blank">
                    https://mowa.clarin-pl.eu
                </a> (wersja starsza)
            </li>
            <li>
                <a href="https://mowa.clarin-pl.eu:8433/" target="_blank">
                    https://mowa.clarin-pl.eu:8433/
                </a> (nowsza wersja w wersji beta)
            </li>
        </ul>

        <p>Nasz zespół składa się z 5 członków:</p>
        <ul>
            <li>
                <b>Prof. dr hab. Krzysztof Marasek</b>
                Kieronik projektu po stronie PJATK
            </li>
            <li>
                <b>dr inż. Danijel Korzinek</b>
                Analiza i rozpoznawanie mowy
            </li>
            <li>
                <b>dr inż. Łukasz Brocki</b>
                Rozpoznawanie mowy
            </li>
            <li>
                <b>dr inż. Krzysztof Wołk</b>
                Tłumaczenie maszynowe
            </li>
            <li>
                <b>mgr inż. Mariusz Kleć</b>
                Wydobywanie informacji muzycznej (MIR), deep learning, web development
            </li>
        </ul>

        <h4 id="korpusy">Korpusy</h4>

        Nasz zespół zajmuje się tworzeniem oraz przetwarzaniem korpusów nagrań mowy. Korpusy służą do wytrenowania określonych modeli oraz wydobywania z nich informacji.

        <h5 id="korpusStudyjny">Korpus Danych Studyjnych</h5>

        <p>
            Jesteśmy autorem korpusu nagrań studyjnych który udostępniamy <a href="https://mowa.clarin-pl.eu/korpusy"
                target="_blank">pod tym linkiem.</a>. Korpus ten posłużył do wytrenowania systemu rozpoznawania mowy dla języka polskiego (bazującego na sytemie Kaldi)
        </p>

        <p>
            Wiele narzędzi wspomnianych wcześniej wymaga dużego zbioru danych trenujących w postaci nagrań audio. Pozyskanie dobrej jakości nagrań mowy w danym języku jest kosztowne i często nieosiągalne przez większość badaczy. Do tej pory nie istniał darmowy, wysokiej jakości korpus nagrań mowy Polskiej z odpowiednio dużym słownikiem.
        </p>

        <p>
            Celem było jak największe zróżnicowanie zarówno pod kątem słownictwa jak również liczby osób czytających aby uchwycić ich różnorodność wymowy. Warto przy tym zwrócić uwagę iż była to mowa czytana, przez co transkrypcja była już dana na samym początku. Korpus stanowi nagrania mowy niejako “wymuszonej”, nie jest to mowa spontaniczna. Teksty były czytane z kartki, dlatego korpus ma większy sens do analizy fonetycznej aniżeli do socjologicznej czy psychologicznej. Korpus ten posłużył do wytrenowania systemu rozpoznawania mowy dla języka polskiego o którym mowa w dalszej części opracowania.
        </p>

        <p>
            Korpus został nagrany w laboratorium dźwiękowym na terenie PJATK z wykorzystaniem mikrofonów studyjnych. W nagraniach brało udział 317 mówców w 554 sesjach, gdzie każda sesja składała się z 20 czytanych zdań oraz 10 fonetycznie bogatych słów pojedynczych. W sumie zostało nagranych 56 godzin mowy składającej się z 356676 słów ze słownika o wielkości 46361. Obecnie korpus jest dostępny do <a
                href="https://ips-lmu.github.io/EMU-webApp/?autoConnect=true&serverUrl=wss:%2F%2Fmowa.clarin-pl.eu:17891%2Fclarin"
                target="_blank">łatwego przeglądania on-line za pośrednictwem systemu EMU-SDMS (EMU Speech Database Management System). </a>Środowisko EMU pozwala na łatwe przeglądanie danych oraz wykonywanie obliczeń statystycznych dzięki integracji z środowiskiem R.
        </p>

        <h5 id="korpusPKF">Korpus PKF</h5>

        <p>
            Korpus Polskich Kronik Filmowych uwzględnia nagrania w formie audio oraz wideo z lat 1945-1962. Korpus charakteryzuje się archaicznym językiem oraz relatywnie niską jakością nagrań co spowodowało iż był wymagającym problemem dla systemu rozpoznawania mowy.  Korpus zawiera 10 minutowe fragmenty wiadomości, podobne do kronik filmowych wyświetlanych w kinie na całym świecie w tym samym okresie. Kronika została stworzona przez Wytwórnie Filmów Dokumentalnych i Fabularnych WFDiF w Warszawie. Obecnie prawa do nagrań należą do Filmoteki Narodowej FINA.
        </p>

        <p>
            Przeważnie jedna kronika zawierała pięć części, każda opisująca inny temat ale wszystkie były czytane przez jednego mówcę. Jedynie okazjonalnie wtrącane były wywiady oraz przemowy innych ludzi. W niektórych przypadkach, takich jak międzynarodowy dzień pracy, cała kronika była podyktowana tylko tym wydarzeniom. Oprócz kin, kroniki były również prezentowane w 1960 roku przez Polską telewizję.
        </p>

        <p>
                PKF są wyjątkowo użytecznym źródłem dla środowiska naukowego z powodu ich bogatej zawartości kulturalnej oraz historycznej. W komunistycznej Polsce kroniki filmowe były często używane jako narzędzie propagandowe. Zawierają komentarze, opinie dziennikarzy, obecne wydarzenia sportowe, ekonomiczne oraz kulturowe z kraju oraz ze świata. Dodatkowo dane mogą być anotowane wieloma meta-informacjami jak czas czy miejsce opisywanych wydarzeń co stanowi wyjątkowo wartościowe źródło do różnych badań społeczno językowych. 
        </p>

        <p>
                
                Było to jednym z powodów dla których zdecydowano się włączyć kroniki do systemu <a
                href="http://chronopress.clarin-pl.eu" target="_blank">ChronoPress</a>. Główną zaletą tego systemu jest dodanie informacji czasowych do korpusu. Podczas gdy standardowy korpus może zawierać materiały z różnych źródeł oraz z różnych okresów czasowych, użycie takiego korpusu może prowadzić do niepoprawnych wniosków. Korpus uporządkowany chronologicznie mówi nam nie tylko jaki jest kontekst wypowiadanych słów ale także kiedy były one używane. Pozwala nam to nie tylko uniknąć wyżej wspomnianego problemu ale także dostarcza wyjątkowego narzędzia do badania rozwoju języka w czasie.
        </p>

        <p>
                Przy okazji korpus posłużył do stworzenia narzędzia automatycznej transkrypcji które może zostać wykorzystane do transkrypcji podobnych nagrań. 
                Aby ułatwić zadanie, zdecydowano iż brane pod uwagę będą jedynie dane z okresu 1945-1962 (aby korelowały z innymi zasobami dostępnymi w ChronoPress) oraz jedynie głos narratora był brany pod uwagę. Jednym z głównych wyzwań w osiągnięciu tego celu był archaiczny język oraz niskiej jakości oraz niestabilny sygnał audio. Dodatkowo, prawie wszystkie filmy cechowała muzyka w tle która w momencie trwania umowy była wyciszona ale podgłaszania gdy było jej brak. 
                Innym problemem było występowanie wielu języków oraz mówców w niektórych filmach. Pomimo tego korpus posiada wiele zalet. Wiele filmów było czytanych przez tą samą grupę dobrze znanych mówców. Dodatkowo domena jest stosunkowo wąska oraz dobrze zdefiniowana. 
                Używając kolekcji przetranskrybowanych kronik, oszacowaliśmy wielkość słownika który wynosił mniej niż 50k wyrazów. W tym celu przetranskrybowane zostały tylko części zawierające komentatora (które stanowiły większość nagrań). Inne części audio (muzyka, pozostali mówcy) zostali ignorowanie.
                
        </p>

        <p>
                Sporym minusem całego projektu była nie możliwość włączenia materiału audio (ze względu na ograniczenia licencyjne). Jedynie transkrypcje mogą zostać opublikowane jako te uzyskane z ramach projektu CLARIN. Jest to duże ograniczenie ponieważ wierzymy że materiał audio mógłby dostarczyć wyjątkowo bogatego źródła do badań fonologii języka Polskiego w tym okresie historycznym.
        </p>
       
        <p>
            Wynikowe transkrypcje zostały włączone do <a href="https://clarin-pl.eu/dspace/handle/11321/426"
                target="_blank">repozytorium projektu CLARIN-PL</a>. Wytrenowany model zwraz z systemem do transkrypcji oraz segmentacji jest przechowywany w formie obrazu <a href="https://hub.docker.com/u/danijel3/"
                target="_blank">Docker</a>, ze szczegółami dostępnymi w <a
                href="https://github.com/speech-clarin-pl/SpeechToolsWorkers/tree/master/speech_tools"
                target="_blank">publicznym repozytorium.</a>
        </p>

        <h5 id="korpusSejm">Korpus Sejm</h5>

        <p>
            Transkrypcja rozmów Parlamentarnych to popularna domena w automatycznym rozpoznawaniu mowy. Rządy wielu krajów wymagają transkrypcji oficjalnych spotkań parlamentarzystów oraz ich upublicznienia. Większość tej pracy jest ciągle wykonywane manualnie. Obecnie obrady Senatu są dostępne online w postaci audio, video oraz tekstowej. 

            Patrząc na przykłady innych, akustycznie podobnych domen do nagrań Parlamentarnych, najbliższe byłyby nagrania ze spotkań gdzie częste przerywanie przez innych oraz mówienie przez kilka osób jest dość częstym zjawiskiem. Z drugiej strony, język oraz używane słowa są kompletnie różne. Bliższym w tym aspekcie byłyby nagrania wykładów. Lecz odbywałyby się w znacząco różnych warunkach akustycznych z prawie nie przerywą przemową jednego mówcy. Widać tutaj iż transkrypcja nagrań sejmowych jest unikatowym zadaniem. 
            
            
            Podzbiór nagrań audio z Senatu, wraz z ich odpowiadającym transkrypcjom, został wybrany do wytrenowania modelu akustycznego oraz językowego. Wiele nagrań zostało opublikowanych na stronie <a href="http://senat.gov.pl" target="blank">Senatu</a>, <a
                href="http://sejm.gov.pl" target="blank">Parlamentu</a> oraz na stronie kanału <a
                href="http://www.tvpparlament.pl/" target="blank">TVP Parlament.</a>
        </p>

        <p>
            Celem było wsparcie już istniejącego systemu prezentującego podpisy filmowe na stronie Polskiego senatu. Strona ta wykorzystuje nagrania wideo oraz transkrypcje jako osobne dokumenty. Nowo stworzony system może być użyteczny jako w pełni funkcjonalne narzędzie do automatycznego wyświetlania transkrypcji, do czasu gdy manualne transkrypcje nie staną się dostępna (zwykle 1-2 dni później). Ponadto może być użyty jako narzędzie segmentacji do przypisywania kodów czasowych do już istniejących manualnych transkrypcji. 
        </p>

        <p>
            Przygotowanie korpusu było czasochłonnym zadaniem, ponieważ oficjalne transkrypcje nie zawierały dokładnych transkrypcji nagrań audio. Były pisane w sposób bardziej odpowiadający poprawności gramatycznej oraz w celu poprawności czytelności pliku PDF niżeli dokładnego odwzorowania mówionych słów. Dlatego też dane musiały być prze-transkrybowane przez grupę zatrudnionych na kontrakcie osób. Usuwali oni również fragmenty audio zawierające duże fragmenty szumu tła, podwójną oraz niezrozumiałą mowę aby ułatwić proces transkrypcji. Duża część danych została również anotowana informacjami o mówcy (nazwa, płeć) aby ułatwić normalizację oraz adaptację modelu akustycznego. 
        </p>

        <p>
            Większość danych zostało pozyskanych dzięki porozumieniu między Polskim Senatem oraz PJATK. Wiele nagrań jest często kiepskiej jakości, realizowanych przy użyciu biurkowych mikrofonów. Wprowadza to wiele zniekształceń, pogłosu oraz szumu tła. Dodatkowo do przetrzymywania nagrań używany jest stratny format plików. 
            W sumie, zostało przygotowanych około 97 godzin nagrań zarówno z Sejmu jak i z Senatu, z 488 różnymi mówcami (w większości mężczyzn). Ze wszystkich zostało wybranych 10 mówców losowo jako zbiór testowy (około 2 godziny nagrań). 
        </p>
       
        <p>
            Pozyskanie wystarczającej ilości danych tekstowych okazał się być nieco bardziej skomplikowany. Elektroniczne korpusy tekstowe są trudne do uzyskania dla języka Polskiego z powodu małej ilości prób cyfryzacji prac pisanych oraz restrykcyjnych praw autorskich. Dodatkowo, nawet jeżeli istnieją tekstowe korpusy, posiadają ograniczony dostęp, zmuszając naukowców do odtwarzania ich własnych zbiorów każdym razem. Początkowo, wystarczająco duży korpus transkrypcji został pozyskany dzięki pomocy biura Senatu, obejmującego ponad 5 milionów słów. Korpus ten został później rozszerzony transkrypcjami z sesji Sejmowych oraz publicznie dostępnej części National Corpus of Polish (NKJP). 
            
            Dane te wymagały segmentacji oraz normalizacji aby rozszerzyć wszystkie liczby oraz skróty do ich mówionych form w odniesieniu do zasad gramatyki. Tego typu normalizacja jest z reguły przeprowadzana manualnie jednak przy takim rozmiarze danych niezbędnym było stworzenie automatycznego narzędzia uzgadniającego formę danego wyrazu. Zostało to rozwiązane przy użyciu autorskiego oprogramowania wytrenowanego na ręcznie przypisanych zasadach gramatycznych przypisanych do rozszerzeń numerycznych. Testy pokazały 10% współczynnik pomyłek. Podsumowując, większość eksperymentów zostało przeprowadzonych na korpusie zawierającym około 145 milionów wyrazów. Korpus został zamieniony na małe litery a wszystkie liczby i skróty rozszerzone do ich pełnych, wypowiadanych gramatycznie form. 
        </p>
      

        <p>
            Największy problem z segmentacją jest taki że manualne transkrypcje nie dokładnie reprezentują co jest aktualnie wypowiadane w nagraniu. Manualne transkrypcje są poprawiane w celu ich łatwiejszego czytania w pliku PDF ale omijają wiele niuansów mowy jako powtórki wyrazów, wtrącenia itp. Dlatego też, została użyta alternatywna technika segmentacji. Po pierwsze audio jest rozpoznane używając modelu języka (wytrenowanego na słowniku który występuje w nagraniu audio) a w drugim kroku używane jest dopasowanie text-to-text aby określić które części audio zostały poprawnie dopasowane do tekstu. Następnie, ta sama metoda jest używana rekursywnie na wszystkich niepoprawnie dopasowanych fragmentacji dopóki nie osiągają perfekcyjnego dopasowania. Ta metoda zawsze kończy się "wymuszonym" dopasowaniem, jednakże nie zawsze jest to nasz cel. Jeżeli transkrypcje które próbujemy dopasować zawierają błędy, możemy chcieć zdecydować aby przerwać "wymuszone" dopasowanie wcześniej i zaakceptować wyjście automatycznego rozpoznawania mowy. 
        </p>

        <p>
            Istnieje wiele technicznych przeszkód w generowaniu podpisów. Rozwiązują to specjalistyczne oprogramowanie, stworzone specjalnie na potrzeby telewizyjne bądź kinowe. Programy te zawierają wiele heurystyk aby określić optymalne dopasowanie informacji na ekranie w celu łatwego ich śledzenia przez użytkownika. System podpisów w projekcie wykorzystującym korpus Sejmowy używa jedynie podstawowych technik (jak np. minimalny czas pomiędzy dwoma następującymi po sobie zmianami itp). Bardziej zaawansowane heurystyki byłyby trudne do zaimplementowania ponieważ wyjściem systemu rozpoznawania mowy jest ciąg wyrazów którym brakuje granic zdań oraz punktuacji. Problem ten jest jednak zaplanowany do rozwiązania w przyszłości. Finalny system posiada także możliwość eksportowania wyników do formatu WebVTT. Jest to standard w3C do opisywania podpisów używanych na stronach internetowych.
        </p>



        <h4 id="narzedziamowy">Narzędzia mowy</h4>

        <h5 id="g2p">Konwersja zapisu ortograficznego na fonetyczny </h5>

        <p>
            Transkrypcja to zapis wymowy danego słowa. Alfabet ortograficzny nie jest w stanie pełnić tej funkcji, ponieważ zapis ortograficzny nie mówi (wbrew pozorom) jak dokładnie należy przeczytać dane słowo. Ponadto mnogość alfabetów (łaciński, cyrylica, koreański i inne) wymagałaby znajomości każdego z nich, by móc przeczytać słowo z danego języka. Trzeba jednak zauważyć, że chociaż istnieje międzynarodowy alfabet fonetyczny (międzynarodowy system API – Alphabet phonétique international), nie zawsze jest on powszechnie stosowany. Międzynarodowy system transkrypcji IPA (International Phonetic Alphabet) był tworzony w oparciu o fonetykę i fonologię języków zachodnioeuropejskich i nie jest zbyt dobrze dostosowany do języka polskiego. 
        </p>

        <p>

        Narzędzie to pozwala na konwersję każdego tekstu napisanego ortograficznie na jego formę fonetyczną (mówioną). Jest to jeden z podstawowych kroków w każdych procesie przetwarzania danych mowy. Narzędzie akceptuje każdą formę tekstu, jednakże nie wykonuje normalizacji tekstu. Oznacza to iż nie zamienia liczb, dat oraz skrótów w sposób automatyczny. 

        System jest stworzony przy użyciu systemu opartego o reguły (972 podstawowe reguły oraz 4802 zastępowań wyrazów z powodu wyjątków). Dlatego też zawiera listę wyjątków dla nazw własnych, zagranicznych i słów nietypowych. Narzędzie może generować zarówno listy wyrazów uwzględniające różne wymowy (z efektu koartykulacji wynikającego z kontekstu) jak również kanoniczną transkrypcję tekstu. 
                
        </p>

        <section class="text-center" style="margin: 25px">
            <img src="images/g2p.png" alt="" style="max-width: 500px" />
            <p>Narzędzie graphene-to-phoneme dostępne na stronie <a href="http://mowa.clarin-pl.eu/tools/ui/phonetize/word" target="_blank">http://mowa.clarin-pl.eu/tools/ui/phonetize/word</a></p>
        </section>

        <p>
                Narzędzie wykorzystuje wariant alfabetu fonetycznego SAMPA, zmodyfikowanego tak aby zawierał tylko litery alfabetu (bez symboli jak apostrof czy tylda które zostały zastąpione literami i oraz n). 
        </p>

        <p>
                Planuje się wdrożenie kilku rozszerzeń do tego narzędzia. Na pierwszym miejscu jest normalizacja tekstu (liczby, daty itp.) przed konwersją. Kolejnym rozszerzeniem jest dołączenie różnych form alfabetu fonetycznego oraz ewentualnie dodanie dodatkowych poziomów anotacji (akcenty lub sylabizacja). Niemniej jednak wdrożenie tych rozszerzeń zależy od zainteresowania środowiska wspomnianymi narzędziami.
        </p>

       

        <h5 id="alignment">Dopasowanie tekstu do audio</h5>
        <p>
            Tzw. "Speech alignment" jest jednym z bardziej użytecznych narzędzi. Jest używany do dopasowania sekwencji słów do dostarczonego nagrania audio zawierającego mowę. Wynik narzędzia może być rozumiany jak automatyczne generowanie kodów czasowych gdy znane są zarówno audio jak i jego transkrypcja. Jest to o tyle użyteczne narzędzie ponieważ może być użyte do łatwego przeszukiwania konkretnych zdarzeń w dużych zbiorach nagrań. Umożliwia takżę obliczanie statystyk odnoszących się do czasu poszczególnych zdarzeń (oraz innych ich charakterystyk).
        </p>

        <section class="text-center" style="margin: 25px">
            <img src="images/segmentacja.png" alt="" style="max-width: 500px"/>
            <p>Narzędzie segmentacji dla krótkich nagrań dostępne na stronie <a href="http://mowa.clarin-pl.eu/tools/ui/align/forced" target="_blank">http://mowa.clarin-pl.eu/tools/ui/align/forced</a>. Oraz dla długich nagrań: <a href="http://mowa.clarin-pl.eu/tools/ui/align/segment" target="_blank">http://mowa.clarin-pl.eu/tools/ui/align/segment</a></p>

            <img src="images/nowasegmentacja.png" alt="" style="max-width: 500px"/>
            <p>Narzędzie segmentacji na nowej stronie www </p>
        </section>

        <p>
            Dopasowanie jest wykonywane zarówno na poziomie słów oraz fonemów. Obecnie narzędzie generuje wyjście w formacie TextGrid (natywnym dla programu Praat) lecz planuje się wdrożenie innych formatów. Narzędzie generuje również link to przeglądarki EMU-webApp która umożliwia przeglądanie rezultatów segmentacji bezpośrednio w przeglądarce www.
        </p>

        <p>
            W ramach rozszerzeń planuje się implementację lepszego modelu akustycznego. Adaptacja modelu akustycznego wraz z modelem języka byłaby również korzystna, szczególnie jeśli chodzi o zaszumione dane. Narzędzie działa poprawnie dla czystych i przewidywalnych danych. Może jednak produkować błędy bądź całkowicie zawieść się dla sygnałów bardzo zaszumionych bądź o niskim poziomie energii. Rozszerzenie interfejsu byłoby również bardzo korzystne. Umożliwiłoby manualną poprawę dopasowania oraz poprawy niektórych informacji (np. transkrypcji ortograficznej bądź fonetycznej). Tego typu usprawnienia zmieniłyby obecny w pełni automatyczne narzędzie na semi-automatyczne.
        </p>

        <h5 id="det">Detekcja mowy</h5>

        <p>
            Detekcja mowy (Voice activity detection: VAD) jest często używane na etapie pre-processingu do wielu narzędzi przetwarzania mowy. Ponieważ dane audio są zwykle nie monogeniczne oraz zawierają zmiksowane fragmenty mowy, muzyki, tła oraz ciszę. Rozróżnienie pomiędzy tymi różnymi typami audio jest niezwykle istotne w skuteczności systemu do transkrypcji. Jego celem jest odizolowanie części zawierających mowę od części zawierających inny typ zdarzeń (cisza, szum, muzyka itp.). Narzędzie to jest kompletnie niezależne od języka oraz domeny wypowiedzi. Niemniej jednak może generować błędy przy bardzo zaszumionych danych. 
        </p>

        <section class="text-center" style="margin: 25px">
                <img src="images/vad.png" alt="" style="max-width: 500px"/>
                <p>Narzędzie do detekcji mowy jest dostępne na stronie <a href="http://mowa.clarin-pl.eu/tools/ui/speech/vad" target="_blank">http://mowa.clarin-pl.eu/tools/ui/speech/vad</a>.</p>
        </section>

        <p>
            Niewielki eksperyment potwierdził wysoki poziom czułości (Recall ~ 99%) oraz średnią precyzję (Precision ~ 58%). Było to jednak zamierzonym celem aby nie utracić żadnych części zawierających mowę, akceptując czasami fragmenty które jej nie zawierają. Jest to spowodowane tym iż pozostałe narzędzia akceptują niewielką ilość zaszumionych danych, ale działają błędnie gdy jakakolwiek część mowy jest pominięta.
        </p>

        <h5 id="vad">Diaryzacja mówców</h5>

        <p>
            Narzędzie to jest używane do segmentacji dużych plików audio na części wypowiadane przez poszczególne osoby. Istnieje kilka typów strategii segmentacji mówców. Pierwsza to rozpoznawanie momentu zmiany mówcy na innego, druga to dodanie informacji który fragment należy do tego samego mówcy oraz trzecia strategia polega na identyfikacji rozpoznanych fragmentów tak aby wiedzieć kto dokładnie mówi w rozpoznanym segmencie. Nasze narzędzie wspiera jednak drugą strategię w której rozpoznajemy zmiany mówców, wiemy ilu ich jest oraz w jakich momentach nagrania występują. Narzędzie jednak traktuje mówców w sposób anonimowy. Narzędzie to jest użyteczne do adaptacji różnych narzędzi oraz modeli do indywidualnych mówców ale również do innych typów analiz które wymagają segmentacji mówców.
        </p>

        <section class="text-center" style="margin: 25px">
                <img src="images/dia.png" alt="" style="max-width: 500px"/>
                <p>Narzędzie diaryzacji jest dostępne na stronie <a href="http://mowa.clarin-pl.eu/tools/ui/speech/diarize" target="_blank">http://mowa.clarin-pl.eu/tools/ui/speech/diarize</a>.</p>
        </section>

        <h5 id="kws">Detekcja słów kluczowych</h5>
        <p>
            Często dokładna transkrypcja materiału audio nie jest konieczna ponieważ jesteśmy zainteresowani tylko występowaniem pojedynczych słów. Wykrywanie słów kluczowych jest procesem który pobiera plik audio oraz listę słów kluczowych. Następnie generuje listę występowania tych słów w obrębie pliku audio. 
            Należy jednak zwrócić uwagę że model języka ma ograniczoną wielkość słownika, dlatego też niemożliwym jest przewidzieć wszystkich możliwych wyrazów. Z tego powodu, system używa kombinacji słów oraz wyrazów w taki sposób że gdy jest potrzeba znalezienia słowa spoza słownika, używana jest reprezentacja sylabowa danego słowa. Dzięki temu system radzi sobie ze słowami które są spoza słownika, ale jest bardziej podatny na błędy gdy dostarczone są bardzo krótkie słowa kluczowe. W celu przetestowania narzędzia został przygotowany test który pokazał całkowitą precyzję na poziomie ~95% oraz poziom czułości (Recall) dla znanych wyrazów ~82% oraz niski poziom dla wyrazów nieznanych (~20%). Model oparty o sylaby wymaga poprawy w przyszłości aby uniknąć błędów w przypadku wyrazów nieznanych.
                
        </p>

        <section class="text-center" style="margin: 25px">
                <img src="images/keyword.png" alt="" style="max-width: 500px"/>
                <p>Narzędzie wykrywania słów kluczowych jest dostępne na stronie <a href="http://mowa.clarin-pl.eu/tools/ui/speech/kws" target="_blank">http://mowa.clarin-pl.eu/tools/ui/speech/kws</a>.</p>
        </section>

        <h5 id="asr">Automatyczne rozpoznawanie mowy</h5>

        <p>Narzędzie to używa systemu rozpoznawania mowy do wygenerowania najbardziej prawdopodobnyej ortograficznej transliteracji nagrań dźwiękowych mowy Polskiej. 

        Schemat poniżej przedstawia zasadę działania obecnego systemu rozpoznawania mowy. 
        
        <section class="text-center" style="margin: 25px">
                <img src="images/asrSchemat.png" alt="" style="max-width: 500px"/>
                <p>Schemat działania systemu rozpoznawania mowy</p>
        </section>
        
        
        Na początku sygnał audio jest poddawany ekstrakcji cech w postaci ramek czasowych. Użyty został standardowy zestaw 39 cech (głównie MFCC) z 100 ramkami na sekundę 25ms nakładaniem się. Następnie ramki są filtrowane używając modułu VAD. Ramki zawierające tylko mowę są następnie poddawane rozpoznawaniu mówców celem adaptacji modelu akustycznego.

        Model akustyczny modeluje prawdopodobieństwo występowania słów na podstawie obserwowanych cech akustycznych. Wyjściem z modelu akustycznego są fonemy które muszą zostać zamienione na wyrazy. Robi to moduł konwersji fonemów na grafemy. Ciągi wyrazów wymagają jednak ułożenia w sekwencję odpowiednią dla gramatyki danego języka. Robi to model językowy który oblicza prawdopodobieństwo sekwencji wyrazów. Dekoder wybiera najwyższe prawdopodobieństwo określonej sekwencji i zwraca jako ciąg słów. 
                
        Słowa te są określone przez słownik który jest mapowany do wyjścia fonetycznego modelu akustycznego. Konwersja P2G jest istotnym pomostem pomiędzy dźwiękiem mowy oraz jak jest używamy podczas czytania i pisania słów na papierze. Procedura ta jest ważna zarówno w fazie trenowania (do konwersji trenujących transkrypcji na fonemy) oraz podczas normalnego użycia (do konwersji fonemów na tekst czytany).
        </p>

        <section class="text-center" style="margin: 25px">
                <img src="images/recstary.png" alt="" style="max-width: 500px"/>
                <p>Narzędzie rozpoznawania mowy jest dostępne na stronie <a href="http://mowa.clarin-pl.eu/tools/ui/speech/kws" target="_blank">http://mowa.clarin-pl.eu/tools/ui/speech/kws</a>.
                
                <img src="images/recnowy.png" alt="" style="max-width: 500px"/>
                <p>Narzędzie rozpoznawania mowy jest również dostępne na nowej stronie www.

                </p>
        </section>



        <h4 id="analizaKorpusowa">Analiza Korpusowa</h4>
        
        Analiza korpusowa jest realizowana przedewszystkim w celach badawczych. Jest to złożony proces, wymagający przedewszytkim dobrze opisanej bazy danych oraz narzędzi do jej przetwarzania. Cały proces budowania korpusów po ich końcową analizę oparliśmy o system EMU-SDMS (EMU - Speech Database Management System). Jest on zbiorem narzędzi których celem jest bycie jak bycie najbliżej rozwiązaniom typu "all-in-one" do generowania, manimulacji, zapytań, analizy oraz zarządzania nagraniami audio zawierającymi mowę.

        <h5 id="przegladarkaEMU">Infrastruktura EMU-SDMS</h5>
        <p>
            System jest skoncentrowany wokół języka statystycznego R. System EMU-SDMS zawiera 4 istotne biblioteki: <code>wrassp, emuR, emuDB, EMU-webApp</code>. Te 4 komponenty pomagają w przeprowadzaniu badań dotyczących mowy. Dzięki nim możemy np. odpowiedzieć na pytanie badawcze: <i>biorąc pod uwagę daną bazę danych, czy na wysokość samogłoski a (mierzoną przez jej pierwszy formant) ma wpły to, czy pojawia się ona w wyrazie czy jako znak funkcyjny?</i>
        </p>

        <section class="text-center" style="margin: 25px">
            <img src="images/overview.png" alt="overview EMU" style="max-width:700px; width: 100%"/>
            <p>Schematyczna architektura systemu EMU-SDMS</p>
        </section>

        <p>
            Trzeba zauważyć że pakiet <code>emuR</code> jest jedynym komponentem który komunikuje się ze wszystkimi innymi. Z tego powodu jest najistotniejszy. Zajmuje się obsługą plików oraz bazy danych. Używa pakietu <code>wrassp</code> do przetwarzania sygnałów oraz dostarcza resultaty do <code>EMU-webApp</code>. Pomimo iż system składa się z 4 komponentów, urzytkownik używa przede wszystkim <code>Emu-webApp</code> oraz <code>emuR</code>.
        </p>
        
        <h5 id="przeplywPracy">Etapy pracy badawczej</h5>
        <p>W przyapadku pracy z systemem EMU-SDMS wygląda on następująco:</p>
        
        <ol>
            <li>Załadowanie bazy danych do sesji R (<code>load_emuDB()</code>)</li>
            <li>Annotacja bazy oraz wizualna inspekcja (<code>serve()</code>). Operacja ta otwiera <code>EMU-webApp</code> w przeglądarce</li>
            <li>Zapytanie do bazy danych (<code>query()</code>). Opcjonalnie przy użyciu funkcji <code>requery_hier()</code> lub <code>requery_seq()</code></li>
            <li>Zbieranie danych śledzących (np. wartości formantów) do wyników zapytania</li>
            <li>Przygotowanie oraz Wizualna inspekcja danych</li>
            <li>Przeprowadzenie dalszych analiz oraz przetwarzanie statystyczne</li>
        </ol>

        
        <p>Początkowo użytkownik tworzy referencję do <code>emuDB</code> poprzez funkcję <code>load_emuDB()</code>. Tak uzyskana referencja do bazy może zostać użyta albo do wyświeltania jej w EMU-webApp (<code>serve()</code>) lub do tworzenia zapytań (<code>query()</code>). Rezultaty zapytania mogą wtedy być użyte albo do wykonania jednego lub więcej tzw. podzapytań (requeries) lub do wyekstrahowania wartości cech sygnału które odpowiadają wynikowi zapytania. 
            
        Finalnie, dane sygnału mogą przejść dalsze przygotowania (np. usuwanie wartości skrajnych tzw. "outliers") oraz sprawdzenie wizualnie przed dalszą analizą. Wynikowy obiekt powyższych funkcji (<code>data.frame</code>) może być użyty jako wejście do tysięcy innych funkcji języka R. EMU-SDMS znacząco redukuje liczbę narzędzi z którymi badacze muszą się zmagać oraz pomaga ułatwić odpowiedzi na pytania badawcze. Jedynym wymogiem w użyciu jest podstawowa znajomość platformy R.</p>

       
        <p>
        Aby lepiej zobrazować zasadę działania systemu, przeprowadzimy wszystkie procesy krok-po-kroku aby odpowiedzieć na pytanie badawcze: <i>mając dany zbiór danych, wysokość samogłoski a jest zależna od tego czy występuje w wyrazie czy jako znak funkcyjny?</i>. UWAGA: poniższe pracowanie dotyczy tekstów angielskich dlatego w rzeczywistości dotyczy samogłoski @.
        </p>

        <h7><b>Ad. 1: Załadowanie bazy danych do sesji R</b></h7>
        <p>
        Gdy nagrania audio są dostępne należy przekształcić jes na format <code>emuDB</code>. Istnieje kilka metod które umożliwią nam zaimportowanie kolekcji plików audio wraz z ich transkrypcją w formacie np. TextGrid. Służy do tego jedna z funkcji <code>convert_ToxtGridCollection()</code>. Oczywiście istnieją również inne ułatwiające ten proces. Ich opis jest dostępny w <a
                href="https://ips-lmu.github.io/The-EMU-SDMS-Manual/chap-tutorial.html"
                target="_blank">dokumentacji</a>. 
        </p>

        <p>
            Proces ten może być jednak zautomatyzowany za pomocą naszego rozwiązania do tworzenia korpusów EMU-SDMS dostępnego pod <a href="http://mowa.clarin-pl.eu/emu/" target="_blank">tym linkiem</a>.

            <section class="text-center" style="margin: 25px">
                    <img src="images/emudesigner.png" alt="overview EMU" style="max-width:700px; width: 100%"/>
                    <p>System do tworzenia korpusów EMU</p>
                </section>

            Po stworzeniu przez powyższego kreatora korpusu EMU, możemy wczytać go do przeglądarki EMU-webApp bądź ściągnąć na dysk lokalny i dalej pracować na nim w środowisku R. 
            Przewagą ściągnięcia korpusu jest to iż korpus lokalny będzie zawierał już obliczone hierarchiczne zależności między poziomami anotacji oraz obliczone cechy sygnału (F0, formanty oraz energia sygnału).
            
            
        </p>
        <p>
                Ściągnięty korpus można wczytać używając funkcji <code>load_emuDB()</code>. Dalej za pomocą funkcji <code>summary()</code> możemy podejrzeć strukturę bazydanych.
        </p>

        <!--
        <p>
        W EMU-SDMS, kolekcja plików, taka jak kolekcja TextGrid, odnosi się do zestawu par w których występują dwa typy plików o różnych rozszerzeniach ale o tej samej nazwie podstawowej. Jest to ważne aby system był w stanie sparować pliki razem ze sobą. Funkcja <code>convert_ToxtGridCollection()</code> przekonwertuje kolekcje TextGrid na format emuDB. Założeniem wstępnym jest to aby pliki .TextGrid zawierały te same poziomy annotacji. Jeżeli warunek nie jest spełniony, można wybrać tylko ten podzbiór poziomów annotacji który jest wspólny dla wszystkich plików. Można oczywiście wybrać które poziomy annotacji chcemy brać pod uwagę. 
        </p>
        -->

       

        <h7><b>Ad. 2: Wizualna inspekcja danych</b></h7>

        <p>
        Po załadowaniu przychodzi czas na wizualną inspekcję danych w EMU-webApp. Służy to tego funkcja <code>serve()</code>. Otwiera ona EMU-webApp w przeglądarce.
        </p> 

        <section class="text-center" style="margin: 25px">
            <img src="images/tutorialEmuWebAppMyFirst.png" alt="overview EMU" style="max-width:700px; width: 100%"/>
            <p>Screen okna EMU-webApp wyświetlający sesję z nagraniami oraz annotacją</p>
        </section>
        
        <h7><b>Ad. 3: Zapytania do bazy danych</b></h7>

        <p>
        Integralnym krokiem w domyślnym przepływie pracy są zapytania do bazy danych. <code>emuR</code> implementuje metodę <code>query()</code> do tego celu. Funkcja ta wykonuje wyrażenia EMU Query Language (EQL) oraz wydobywa zanotowane fragmenty z bazy danych które spełniają wymagania zapytania. Możemy wykonywać zapytania osobno dla każdego poziomu np. aby zapytanie zwróciło wszystkie fragmenty z poziomu "Syllabe", które są równe "S" (silna sylaba). W tym celu wykonaliśmy polecenie:

      
        <figure>
            <code>
                sl_syl = query(emuDBhandle = db_handle, &quot;Syllable == S&quot;)
            </code>
        </figure>

        Rezultatem takiego zapytania jest obiekt <code>emuRsegs</code>, który jest superklasą obiektu <code>data.frame</code>. Obiekt ten często odnosi się do listy segmentów które są opisane za pomocą punktu startowego, punktu końcowego, z jakiej sesji oraz paczki pochodzi dany wpis oraz poziom do którego należy.
        
        </p>

    <p>
            Tego typu zapytania pochodzą z pojedyńczego poziomu który pasuje do określonej etykiety. Jednakże EMU-SDMS oferuje mechanizm do wykonywania zapytań wewnątrz poziomowych takich jak: <i>zwróć wszystkie pozycje z poziomu "Phonetic", które zawierają etykietę "n" oraz są częścią zawartości wyrazu</i>. Aby to było możliwe, EMU-SDMS oferuje dość zaawansowane funkcje modelowania struktury adnotacji. Aby to było jednak możliwe, należy przekształcić płaską strukturę annotacji na hierarchiczną jak zaprezentowano na poniższym rysunku.

            <section class="text-center" style="margin: 25px">
                <img src="images/tutorial-violentlyHier-1.png" alt="" style="max-width:700px; width: 100%" />
                <p>Przykład hybrydowej annotacji zawierającej informację czasową (poziom "Phonetic") oraz annotację hierarchiczną (Phoneme, Syllabe, Text uwzględniającą ich wewnętrzne powiązania).</p>
            </section>
            

            Warto zauważyć że tego typu połączenia hierarchiczne są obliczane automatycznie w naszym narzędziu do tworzenia korpusów EMU.

        </p>
        
        <p>
            Po utworzeniu struktury można wyświetlić ja w Emu-webApp dzięki komendzie <code>serve()</code>. W Emu-webApp możemy zobaczyć stworzoną strukturę klikając przycisk "show hierarchy":
        </p>

        <section class="text-center" style="margin: 25px">
            <img src="images/EMU-webAppScreenshotTutorialPostAutobHier.png" alt="" style="max-width:700px; width: 100%" />
            <p>Przykład EMU-webApp wyświetlający automatycznie zbudowaną hierarchię pomiędzy poziomami annotacji.</p>
        </section>
        
        <p>
            Hierarchiczna struktura annotacji pozwala na formułowanie zapytań które pomogą odpowiedzieć na bardziej złożone pytania badawcze np. <i>wysokość samogłoski @ różni się w zależności od tego czy występuje w wyrazie czy jako znak funkcyjny. </i>
           
        </p>

        <p>
            Na początku należy wydobyć wszystkie samogłoski @ z bazy danych z poziomu "Phonetic":
        </p>

        <figure>
            <code>
                sl_vowels = query(db_handle, &quot;Phonetic == @&quot;)
            </code>
        </figure>

        <p>
            Potrzebujemy jednak typu wyrazu (zwykły bądź funkcyjny) dla którego zostały znalezione samogłoski @. W tym celu użyjemy funkcjonalności zwanej "requery" z systemu EMU-SDMS. Dzięki temu wydobędziemy typ wyrazu odpowiadający każdej samogłosce @. W skrócie, "requery" porusza się przez hierarchiczną annotacje (vertykalnie i horyzontalnie) zaczynając od segmentów które zostały przekazane do funkcji jako parametry.
        </p>

        <figure>
            <code>
                sl_word_type = requery_hier(db_handle, seglist = sl_vowels, level = &quot;Word&quot;, calcTimes = FALSE)
            </code>
        </figure>

     

        <p>
            Należy zwrócić uwagę że liczba wierszy w liście segmentów zwróconych przez funkcję <code>requery_hier()</code> jest taka sama jak oryginalna lista samogłosek sl_vowels. Jest to istotne, ponieważ każdy rząd w obu listach segmentów powinien być połączony, pozwalając nam na dopasowanie który segment należy do jakiego typu wyrazu.
        </p>


        <h7><b>Ad. 4: Ekstrakcja cech audio</b></h7>
        <p>
            Jeżeli mamy już samogłoski oraz typu wyrazów im odpowiadające (oraz informacje o ich czasie występowania), możemy wyekstrachować cechy sygnału do tych segmentów. Funkcja <code>get_trackdata()</code> oblicza wartości formantów w czasie rzeczywistym, używając funkcji estymacji formantów <code>forest()</code>, dostarczonej przez pakiet <code>wrassp</code>.
        </p>

        <figure>
            <code>
               
                 td_vowels = get_trackdata(db_handle, seglist = sl_vowels, onTheFlyFunctionName = &quot;forest&quot;, resultType = &quot;tibble&quot;, verbose = F)
            </code>
        </figure>
    
        <p>
            funkcja <code>forest()</code> oblicza 4 wartości formantów. Interesują nas jednak tylko wartości F1 oraz F2. </p>

        <h7><b>Ad. 5 i 6: Wizualna inspekcja danych</b></h7>

        <p>
            Możemy narysować wykres obrazujący trajektorie pierwszego formantu dla wszystkich znalezionych segmentów:
        </p>

        

        <section class="text-center" style="margin: 25px">
            <img src="images/plot1.png" alt="" style="max-width:700px; width: 100%" />
            <img src="images/tutorial-dplot1-1.png" alt="" style="max-width:700px; width: 100%" />
            <p>Trajektoria formantów F1 dla wszystkich znalezionych samogłosek @</p>
        </section>
        

        <p>
            Jednak aby wyczyścić ten wykres, kolejny zawiera normalizację długości segmentu oraz metodę wygładzającą <code>geom_smooth()</code> aby wyświetlić wygładzoną średnią warunkową wszystkich samogłosek @
        </p>

       

        <section class="text-center" style="margin: 25px">
            <img src="images/plot2.png" alt="" style="max-width:700px; width: 100%" />
            <img src="images/tutorial-dplot2-1.png" alt=""  style="max-width:700px; width: 100%"/>
            <p>Uśredniona i znormalizowana trajektoria formantów F1 dla wszystkich znalezionych samogłosek @</p>
        </section>
        

        <p>
            Powyższe wykresy dają wgląd w trajektorie wszystkich samogłosek @. Do celów eksploracji danych oraz aby uzyskać informację gdzie poszczególne klasy samogłosek leżą w przestrzeni 1 i 2 formantu (które pośrednio dostarczają informacji na temat wysokości samogłoski oraz pozycji języka), możemy użyć ponownie funkcji <code>ggplot()</code>. Jednakże tym razem nie chcemy mieć już trajektorii wartości formantów ale zależy nam na dwówymiarowych danych aby wyświelić je w przestrzeni F2 x F1. Można to uzyskać np. poprzez ekstrakcję środkowego formantu z każdego segmentu (albo za pomocą parametru <code>>cut</code> dla funckcji <code>
            get_trackdata()</code> lub używajć funkcji <code>filter()</code>).
        </p>

       

        <section class="text-center" style="margin: 25px">

            <img src="images/plot3.png" alt="" style="max-width:700px; width: 100%" />
            <img src="images/tutorial-eplot-1.png" alt="" style="max-width:700px; width: 100%" />
            <p>95% elipsa zawierająca centroid dla danych F2 x F1, wyekstrachowanych segmentów samogłosek (z ich środkowych części)</p>
        </section>

        

        <p>
            Wykres powyżej pokazuje pierwsze dwa formanty wyekstrachowane z środka każdej samogłoski @. Centroid tych formantów został pokazany na wykresie F2 x F1 oraz ich 95% rozkład. 
        </p>

        <p>
            Powyższe przykłady nie pomagają wprost odpowiedzieć na pytanie zawarte we wstępie, jednak pokazują użyteczność pakietów R jako użyteczne narzędzia do dostarczania wglądu w dane.
            Aby lepiej zrozumieć język zapytań oraz możliwości generowania wykresów najlepiej przeczytać dokumentację dostępną pod <a href="https://ips-lmu.github.io/The-EMU-SDMS-Manual/app-chap-eql.html" target="_blank">tym linkiem</a>.
        </p>

        <h7><b>7: Dalsze analizy i wnioskowanie statystyczne</b></h7>

        <p>

            Powyższe analizy dotyczyły jedynie samogłoski @ bez odniesienia do typu wyrazu w którym występowała. Jednakże pytanie badawcze uwzględnia ten kontekst. Możemy więc wyekstrachować 60% formantów z ich trajektorii oraz wyświetlić na wykresie. Jednakże
            chcemy je pogrupować ze względu na typ wyrazu w którym występują. W tym celu zastąpimy etykiety <code>td_vowels_mid_sec</code> etykietami <code>>sl_word_type</code>:
        </p>


        <section class="text-center" style="margin: 25px">

            <img src="images/plot4.png" alt="" style="max-width:700px; width: 100%" />

            <img src="images/tutorial-dplotSylTyp-1.png" alt="" style="max-width:700px; width: 100%; margin: 0 auto;" />
            <p>Uśrednione kontury formantów F1 wszystkich samogłosek pogrupowanych ze względu na typ wyrazu (funkcyjny F lub zwykły C)</p>
        </section>
      

        <p>

            Jak można zauważyć, istnieje różnica między wysokością F1 samogłosek występujących samodzielnie oraz w wyrazie. Aby lepiej to zobrazować możemy wygenerować inny wykres
        </p>

        <section class="text-center" style="margin: 25px">

            <img src="images/plot5.png" alt="" style="max-width:700px; width: 100%" />

            <img src="images/tutorial-boxplot-1.png" alt="" style="max-width:700px; width: 100%"/>
            <p>Wykres typu boxplot pokazujący różnicęw fozkładzie F1 w zależności od funkcji samogłoski (jako cześć wyrazu (C) lub jako znak funkcyjny (F)).</p>
        </section>

        <p>
            W dalszych krokach można przeprowadzać dalsze wnioskowanie i testowanie hipotez statystycznych. Powyższy opis jest jednak bardzo ogólny i ma na celu jedynie przybliżenie intuicji w jaki sposób działa system EMU-SDMS. 
        </p>

        <div class="alert alert-primary" role="alert">
                Więcej przykładów oraz wyczerpujący opis użycia EMU-SDMS do rozwiązywania problemów badawczych można znaleźć
            w <a href="https://ips-lmu.github.io/The-EMU-SDMS-Manual/chap-tutorial.html"
                target="_blank">dokumentacji</a>.
        </div>


        <h4 id="przykladyAnaliz">Przykłady analiz</h4>

        <p>Poniższe przykłady odnoszą się do Zeszytów emuR, dostępnymi pod adresem: <a href="https://danijel3.github.io/emuR_notebooks/" target="_blank">https://danijel3.github.io/emuR_notebooks/</a></p>

        <h5 id="przyklad1">Wizualizacja samogłosek</h5>
        <p>Zeszyt dostępny pod adresem: <a href="https://danijel3.github.io/emuR_notebooks/samogloski.nb.html" target="_blank">https://danijel3.github.io/emuR_notebooks/samogloski.nb.html</a></p>

        <h5 id="przyklad2">F0 zależne od płci</h5>
        <p>Zeszyt dostępny pod adresem: <a href="https://danijel3.github.io/emuR_notebooks/pitch.nb.html" target="_blank">https://danijel3.github.io/emuR_notebooks/pitch.nb.html</a></p>

        <h5 id="przyklad3">Głoski dźwięczne i bezdźwięczne</h5>
        <p>Zeszyt dostępny pod adresem: <a href="https://danijel3.github.io/emuR_notebooks/dzwiecznosc.nb.html" target="_blank">https://danijel3.github.io/emuR_notebooks/dzwiecznosc.nb.html</a></p>


        <h5 id="przyklad4">VSA i VAI</h5>
        <p>Zeszyt dostępny pod adresem: <a href="https://danijel3.github.io/emuR_notebooks/VSA_VAI.nb.html" target="_blank">https://danijel3.github.io/emuR_notebooks/VSA_VAI.nb.html</a></p>



    </div>


    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="/docs/4.3/assets/js/vendor/jquery-slim.min.js"><\/script>')</script>
    <script src="/docs/4.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-xrRywqdh3PHs8keKZN+8zzc5TX0GRTLCcmivcbNJWm2rs5C8PRhcEn3czEjhAO9o"
        crossorigin="anonymous"></script>
</body>

</html>